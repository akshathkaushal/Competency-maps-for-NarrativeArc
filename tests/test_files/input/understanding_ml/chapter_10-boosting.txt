10

Boosting

Boosting is an algorithmic paradigm that grew out of a theoretical question and
became a very practical machine learning tool. The boosting approach uses a gen-
eralization of linear predictors to address two major issues that have been raised
earlier in the book. The ﬁrst is the bias-complexity tradeoff. We have seen (in
Chapter 5) that the error of an ERM learner can be decomposed into a sum of
approximation error and estimation error. The more expressive the hypothesis class
the learner is searching over, the smaller the approximation error is, but the larger
the estimation error becomes. A learner is thus faced with the problem of picking a
good tradeoff between these two considerations. The boosting paradigm allows the
learner to have smooth control over this tradeoff. The learning starts with a basic
class (that might have a large approximation error), and as it progresses the class
that the predictor may belong to grows richer.

The second issue that boosting addresses is the computational complexity of
learning. As seen in Chapter 8, for many interesting concept classes the task of
ﬁnding an ERM hypothesis may be computationally infeasible. A boosting algo-
rithm ampliﬁes the accuracy of weak learners. Intuitively, one can think of a weak
learner as an algorithm that uses a simple “rule of thumb” to output a hypothesis
that comes from an easy-to-learn hypothesis class and performs just slightly better
than a random guess. When a weak learner can be implemented efﬁciently, boost-
ing provides a tool for aggregating such weak hypotheses to approximate gradually
good predictors for larger, and harder to learn, classes.

In this chapter we will describe and analyze a practically useful boosting algo-
rithm, AdaBoost (a shorthand for Adaptive Boosting). The AdaBoost algorithm
outputs a hypothesis that is a linear combination of simple hypotheses. In other
words, AdaBoost relies on the family of hypothesis classes obtained by composing
a linear predictor on top of simple classes. We will show that AdaBoost enables us
to control the tradeoff between the approximation and estimation errors by varying
a single parameter.

AdaBoost demonstrates a general theme, that will recur later in the book, of
expanding the expressiveness of linear predictors by composing them on top of
other functions. This will be elaborated in Section 10.3.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

101

102

Boosting

AdaBoost stemmed from the theoretical question of whether an efﬁcient weak
learner can be “boosted” into an efﬁcient strong learner. This question was raised by
Kearns and Valiant in 1988 and solved in 1990 by Robert Schapire, then a graduate
student at MIT. However, the proposed mechanism was not very practical. In 1995,
Robert Schapire and Yoav Freund proposed the AdaBoost algorithm, which was
the ﬁrst truly practical implementation of boosting. This simple and elegant algo-
rithm became hugely popular, and Freund and Schapire’s work has been recognized
by numerous awards.

Furthermore, boosting is a great example for the practical impact of learning the-
ory. While boosting originated as a purely theoretical problem, it has led to popular
and widely used algorithms. Indeed, as we shall demonstrate later in this chapter,
AdaBoost has been successfully used for learning to detect faces in images.

10.1 WEAK LEARNABILITY
Recall the deﬁnition of PAC learning given in Chapter 3: A hypothesis class, H,
is PAC learnable if there exist mH : (0,1)2 → N and a learning algorithm with the
following property: For every (cid:2), δ ∈ (0,1), for every distribution D over X , and for
every labeling function f :X → {±1}, if the realizable assumption holds with respect
to H,D, f , then when running the learning algorithm on m ≥ mH((cid:2), δ) i.i.d. examples
generated by D and labeled by f , the algorithm returns a hypothesis h such that,
with probability of at least 1− δ, L(D, f )(h) ≤ (cid:2).

Furthermore, the fundamental theorem of learning theory (Theorem 6.8 in
Chapter 6) characterizes the family of learnable classes and states that every PAC
learnable class can be learned using any ERM algorithm. However, the deﬁnition of
PAC learning and the fundamental theorem of learning theory ignores the compu-
tational aspect of learning. Indeed, as we have shown in Chapter 8, there are cases in
which implementing the ERM rule is computationally hard (even in the realizable
case).
However, perhaps we can trade computational hardness with the requirement
for accuracy. Given a distribution D and a target labeling function f , maybe there
exists an efﬁciently computable learning algorithm whose error is just slightly better
than a random guess? This motivates the following deﬁnition.

Deﬁnition 10.1 (γ -Weak-Learnability).
(cid:2) A learning algorithm, A, is a γ -weak-learner for a class H if there exists a func-
tion mH : (0,1) → N such that for every δ ∈ (0,1), for every distribution D over
X , and for every labeling function f : X → {±1}, if the realizable assumption
holds with respect to H,D, f , then when running the learning algorithm on m ≥
mH(δ) i.i.d. examples generated by D and labeled by f , the algorithm returns a
hypothesis h such that, with probability of at least 1− δ, L(D, f )(h) ≤ 1/2− γ .
(cid:2) A hypothesis class H is γ -weak-learnable if there exists a γ -weak-learner for

that class.

This deﬁnition is almost identical to the deﬁnition of PAC learning, which here
we will call strong learning, with one crucial difference: Strong learnability implies
the ability to ﬁnd an arbitrarily good classiﬁer (with error rate at most (cid:2) for an

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

10.1 Weak Learnability

103

(cid:2)

d+log(1/δ)

arbitrarily small (cid:2) > 0). In weak learnability, however, we only need to output a
hypothesis whose error rate is at most 1/2− γ , namely, whose error rate is slightly
better than what a random labeling would give us. The hope is that it may be easier
to come up with efﬁcient weak learners than with efﬁcient (full) PAC learners.
The fundamental theorem of learning (Theorem 6.8) states that if a hypothesis
class H has a VC dimension d, then the sample complexity of PAC learning H satis-
ﬁes mH((cid:2), δ)≥ C1
, where C1 is a constant. Applying this with (cid:2) = 1/2−γ we
immediately obtain that if d = ∞ then H is not γ -weak-learnable. This implies that
from the statistical perspective (i.e., if we ignore computational complexity), weak
learnability is also characterized by the VC dimension of H and therefore is just as
hard as PAC (strong) learning. However, when we do consider computational com-
plexity, the potential advantage of weak learning is that maybe there is an algorithm
that satisﬁes the requirements of weak learning and can be implemented efﬁciently.
One possible approach is to take a “simple” hypothesis class, denoted B, and to
apply ERM with respect to B as the weak learning algorithm. For this to work, we
need that B will satisfy two requirements:
(cid:2) ERMB is efﬁciently implementable.
(cid:2) For every sample that is labeled by some hypothesis from H, any ERMB

hypothesis will have an error of at most 1/2− γ .

Then, the immediate question is whether we can boost an efﬁcient weak learner
into an efﬁcient strong learner. In the next section we will show that this is indeed
possible, but before that, let us show an example in which efﬁcient weak learnability
of a class H is possible using a base hypothesis class B.
Example 10.1 (Weak Learning of 3-Piece Classiﬁers Using Decision Stumps). Let
X = R and let H be the class of 3-piece classiﬁers, namely, H = {hθ1,θ2,b : θ1, θ2 ∈
(cid:5)
R, θ1 < θ2,b ∈ {±1}}, where for every x,
+b
−b

if x < θ1 or x > θ2
if θ1 ≤ x ≤ θ2

hθ1,θ2,b(x) =

An example hypothesis (for b = 1) is illustrated as follows:

+

−

+

θ 1

θ 2

Let B be the class of Decision Stumps, that is, B ={x (cid:29)→ sign(x − θ)·b : θ ∈ R,b ∈
{±1}}. In the following we show that ERMB is a γ -weak learner for H, for γ = 1/12.
To see that, we ﬁrst show that for every distribution that is consistent with H,
there exists a decision stump with LD(h) ≤ 1/3. Indeed, just note that every classiﬁer
in H consists of three regions (two unbounded rays and a center interval) with alter-
nate labels. For any pair of such regions, there exists a decision stump that agrees
with the labeling of these two components. Note that for every distribution D over
R and every partitioning of the line into three such regions, one of these regions
must have D-weight of at most 1/3. Let h ∈ H be a zero error hypothesis. A decision
stump that disagrees with h only on such a region has an error of at most 1/3.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

104

Boosting

Finally, since the VC-dimension of decision stumps is 2, if the sample size is
greater than (cid:6)(log(1/δ)/(cid:2)2), then with probability of at least 1− δ, the ERMB rule
returns a hypothesis with an error of at most 1/3+ (cid:2). Setting (cid:2) = 1/12 we obtain that
the error of ERMB is at most 1/3+ 1/12= 1/2− 1/12.
We see that ERMB is a γ -weak learner for H. We next show how to implement

the ERM rule efﬁciently for decision stumps.

LD(h) = m(cid:7)

10.1.1 Efﬁcient Implementation of ERM for Decision Stumps
Let X = Rd and consider the base hypothesis class of decision stumps over Rd,
namely,

HDS = {x (cid:29)→ sign(θ − xi)· b : θ ∈ R,i ∈ [d],b ∈ {±1}}.

For simplicity, assume that b= 1; that is, we focus on all the hypotheses in HDS of the
form sign(θ − xi). Let S = ((x1, y1), . . . ,(xm , ym)) be a training set. We will show how
to implement an ERM rule, namely, how to ﬁnd a decision stump that minimizes
L S(h). Furthermore, since in the next section we will show that AdaBoost requires
(cid:2)
ﬁnding a hypothesis with a small risk relative to some distribution over S, we will
show here how to minimize such risk functions. Concretely, let D be a probability
i Di = 1). The weak
vector in Rm (that is, all elements of D are nonnegative and
learner we describe later receives D and S and outputs a decision stump h : X → Y
that minimizes the risk w.r.t. D,

Di 1[h(xi )(cid:18)=yi ].
Note that if D = (1/m, . . . ,1/m) then LD(h) = L S(h).
(cid:7)

i=1

Recall that each decision stump is parameterized by an index j ∈ [d] and a

threshold θ. Therefore, minimizing LD(h) amounts to solving the problem

min
j∈[d]

min
θ∈R

Di 1[xi, j >θ] +

Di 1[xi, j ≤θ]

i:yi=−1

⎛
⎝(cid:7)

i:yi=1

⎞
⎠.

(10.1)

2

Fix j ∈ [d] and let us sort the examples so that x1, j ≤ x2, j ≤ . . . ≤ xm, j . Deﬁne
(cid:14) j = { xi, j+xi+1, j
: i ∈ [m − 1]}∪ {(x1, j − 1),(xm, j + 1)}. Note that for any θ ∈ R there
exists θ(cid:3) ∈ (cid:14) j that yields the same predictions for the sample S as the threshold θ.
Therefore, instead of minimizing over θ ∈ R we can minimize over θ ∈ (cid:14) j .
This already gives us an efﬁcient procedure: Choose j ∈ [d] and θ ∈ (cid:14) j that
minimize the objective value of Equation (10.1). For every j and θ ∈ (cid:14) j we have to
calculate a sum over m examples; therefore the runtime of this approach would be
O(dm2). We next show a simple trick that enables us to minimize the objective in
time O(dm).
The observation is as follows. Suppose we have calculated the objective for θ ∈
(xi−1, j , xi , j ). Let F(θ) be the value of the objective. Then, when we consider θ(cid:3) ∈
(xi , j , xi+1, j ) we have that

F(θ(cid:3)

) = F(θ)− Di 1[yi=1] + Di 1[yi=−1] = F(θ)− yi Di .

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

10.2 AdaBoost

105

Therefore, we can calculate the objective at θ(cid:3)
in a constant time, given the objective
at the previous threshold, θ. It follows that after a preprocessing step in which we
sort the examples with respect to each coordinate, the minimization problem can be
performed in time O(dm). This yields the following pseudocode.

ERM for Decision Stumps

input:

training set S = (x1, y1), . . . ,(xm , ym)
distribution vector D

goal: Find j (cid:7), θ (cid:7) that solve Equation (10.1)
initialize: F (cid:7) = ∞
for j = 1, . . . ,d

sort S using the j’th coordinate, and denote
x1, j ≤ x2, j ≤ ··· ≤ xm, j ≤ xm+1, j
def= xm, j + 1

F =(cid:2)

i:yi=1 Di

if F < F (cid:7)
F (cid:7) = F, θ (cid:7) = x1, j − 1, j (cid:7) = j
for i = 1, . . . ,m
F = F − yi Di
if F < F (cid:7) and xi , j (cid:18)= xi+1, j
F (cid:7) = F, θ (cid:7) = 1

2(xi , j + xi+1, j ), j (cid:7) = j

output j (cid:7), θ (cid:7)

10.2 ADABOOST
AdaBoost (short for Adaptive Boosting) is an algorithm that has access to a weak
learner and ﬁnds a hypothesis with a low empirical risk. The AdaBoost algorithm
receives as input a training set of examples S = (x1, y1), . . . ,(xm , ym), where for
each i, yi = f (xi) for some labeling function f . The boosting process proceeds in
a sequence of consecutive rounds. At round t, the booster ﬁrst deﬁnes a distribution
over the examples in S, denoted D(t). That is, D(t) ∈ Rm+ and
= 1. Then,
the booster passes the distribution D(t) and the sample S to the weak learner. (That
way, the weak learner can construct i.i.d. examples according to D(t) and f .) The
weak learner is assumed to return a “weak” hypothesis, ht , whose error,

i=1 D(t)

(cid:2)

m

i

def= LD(t)(ht ) def= m(cid:7)

(cid:2)t

i=1

D(t)
i

1[ht (xi )(cid:18)=yi ],

− γ (of course, there is a probability of at most δ that the weak learner
is at most 1
2
fails). Then, AdaBoost assigns a weight for ht as follows: wt = 1
. That
is, the weight of ht is inversely proportional to the error of ht . At the end of the
round, AdaBoost updates the distribution so that examples on which ht errs will
get a higher probability mass while examples on which ht is correct will get a lower
probability mass. Intuitively, this will force the weak learner to focus on the prob-
lematic examples in the next round. The output of the AdaBoost algorithm is a

− 1

2 log

1
(cid:2)t

(cid:16)

(cid:17)

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

106

Boosting

“strong” classiﬁer that is based on a weighted sum of all the weak hypotheses. The
pseudocode of AdaBoost is presented in the following.

AdaBoost

input:

m

training set S = (x1, y1), . . . ,(xm , ym)
weak learner WL
number of rounds T

m ).

, . . . , 1

initialize D(1) = ( 1
for t = 1, . . . , T :
compute (cid:2)t =(cid:2)
invoke weak learner ht = WL(D(t), S)
(cid:16)
(cid:17)
i=1 D(t)
let wt = 1
− 1
1
2 log
(cid:2)t
i exp(−wt yi ht (xi ))
update D(t+1)
= D(t)
(cid:2)
j=1 D(t)
output the hypothesis hs(x) = sign

j exp(−wt y j ht (x j ))
T
t=1

1[yi(cid:18)=ht (xi )]

(cid:16)(cid:2)

m

m

i

i

for all i = 1, . . . ,m

(cid:17)

wt ht (x)

.

The following theorem shows that the training error of the output hypothesis

decreases exponentially fast with the number of boosting rounds.

Theorem 10.2. Let S be a training set and assume that at each iteration of AdaBoost,
the weak learner returns a hypothesis for which (cid:2)t ≤ 1/2− γ . Then, the training error
of the output hypothesis of AdaBoost is at most

m(cid:7)
Proof. For each t, denote ft =(cid:2)

L S(hs) = 1
m

i=1

fT . In addition, denote

1[hs(xi )(cid:18)=yi ] ≤ exp(− 2 γ 2 T ) .

p≤t

wph p. Therefore, the output of AdaBoost is

m(cid:7)

i=1

Zt = 1
m

−yi ft (xi ).

e

Note that for any hypothesis we have that 1[h(x)(cid:18)=y] ≤ e
Z T , so it sufﬁces to show that Z T ≤ e

−2γ 2T . To upper bound Z T we rewrite it as

−yh(x). Therefore, L S( fT ) ≤

(10.2)
where we used the fact that Z0 = 1 because f0 ≡ 0. Therefore, it sufﬁces to show that
for every round t,

,

= Z T
Z T−1

· Z T−1
Z T−2

··· Z2
Z1

· Z1
Z0

Z T = Z T
Z0

Zt+1
Zt

≤ e

−2γ 2.

(10.3)

To do so, we ﬁrst note that using a simple inductive argument, for all t and i,

D(t+1)

i

=

(cid:2)

−yi ft (xi )
e
m
j=1 e

−y j ft (x j ) .

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

Hence,

Zt+1
Zt

m
i=1 e
e

j=1
m
i=1 e

=

(cid:2)
m(cid:2)
(cid:2)
= m(cid:7)

=

i=1
= e
−wt+1

−yi ft+1(xi )
−y j ft (x j )

m(cid:2)
−yi ft (xi )e

j=1
D(t+1)

e

i

(cid:7)

−yi wt+1ht+1(xi )

−y j ft (x j )

e

−yi wt+1ht+1(xi )

10.2 AdaBoost

107

D(t+1)

i

(cid:7)

i

=

D(t+1)

i:yi ht+1(xi )=1

i:yi ht+1(xi )=−1

(1− (cid:2)t+1)+

+ ewt+1
(cid:22)
−wt+1(1− (cid:2)t+1)+ ewt+1 (cid:2)t+1
= e
1(cid:22)
(1− (cid:2)t+1)+
1/(cid:2)t+1 − 1 (cid:2)t+1
=
+
(cid:23)
1/(cid:2)t+1 − 1
1− (cid:2)t+1
(cid:2)t+1
(cid:22)
1− (cid:2)t+1
(cid:2)t+1
(cid:2)t+1(1− (cid:2)t+1).
By our assumption, (cid:2)t+1 ≤ 1
+(cid:20)
increasing in [0,1/2], we obtain that

(cid:21)
=
(cid:22)
1− 4γ 2 ≤ e
Finally, using the inequality 1 − a ≤ e
This shows that Equation (10.3) holds and thus concludes our proof.

(cid:22)
(cid:2)t+1(1− (cid:2)t+1) ≤ 2

1
2
−a we have that

(cid:31)
1− 4γ 2.

(cid:21)(cid:20)

= 2

+ γ

− γ

(cid:2)t+1

1
2

2

2

− γ . Since the function g(a)= a(1− a) is monotonically

−4γ 2/2 = e

−2γ 2.

Each iteration of AdaBoost involves O(m) operations as well as a single call to
the weak learner. Therefore, if the weak learner can be implemented efﬁciently (as
happens in the case of ERM with respect to decision stumps) then the total training
process will be efﬁcient.
Remark 10.2. Theorem 10.2 assumes that at each iteration of AdaBoost, the weak
learner returns a hypothesis with weighted sample error of at most 1/2− γ . Accord-
ing to the deﬁnition of a weak learner, it can fail with probability δ. Using the union
bound, the probability that the weak learner will not fail at all of the iterations is at
least 1− δT . As we show in Exercise 10.1, the dependence of the sample complex-
ity on δ can always be logarithmic in 1/δ, and therefore invoking the weak learner
with a very small δ is not problematic. We can therefore assume that δT is also
small. Furthermore, since the weak learner is only applied with distributions over
the training set, in many cases we can implement the weak learner so that it will have
a zero probability of failure (i.e., δ = 0). This is the case, for example, in the weak

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

108

Boosting

learner that ﬁnds the minimum value of LD(h) for decision stumps, as described in
the previous section.

Theorem 10.2 tells us that the empirical risk of the hypothesis constructed by
AdaBoost goes to zero as T grows. However, what we really care about is the true
risk of the output hypothesis. To argue about the true risk, we note that the output
of AdaBoost is in fact a composition of a halfspace over the predictions of the T
weak hypotheses constructed by the weak learner. In the next section we show that
if the weak hypotheses come from a base hypothesis class of low VC-dimension,
then the estimation error of AdaBoost will be small; namely, the true risk of the
output of AdaBoost would not be very far from its empirical risk.

,

10.3 LINEAR COMBINATIONS OF BASE HYPOTHESES
As mentioned previously, a popular approach for constructing a weak learner is to
apply the ERM rule with respect to a base hypothesis class (e.g., ERM over decision
stumps). We have also seen that boosting outputs a composition of a halfspace over
the predictions of the weak hypotheses. Therefore, given a base hypothesis class B
(e.g., decision stumps), the output of AdaBoost will be a member of the following
class:

(cid:5)

(cid:26)

T(cid:7)

(cid:27)

L(B, T ) =

x (cid:29)→ sign

: w ∈ RT , ∀t, ht ∈ B

wt ht (x)

(10.4)
That is, each h ∈ L(B, T ) is parameterized by T base hypotheses from B and by
a vector w ∈ RT . The prediction of such an h on an instance x is obtained by ﬁrst
applying the T base hypotheses to construct the vector ψ(x) = (h1(x), . . . ,hT (x)) ∈
RT , and then applying the (homogenous) halfspace deﬁned by w on ψ(x).

t=1

.

In this section we analyze the estimation error of L(B, T ) by bounding the VC-
dimension of L(B, T ) in terms of the VC-dimension of B and T. We will show that,
up to logarithmic factors, the VC-dimension of L(B, T ) is bounded by T times the
VC-dimension of B. It follows that the estimation error of AdaBoost grows linearly
with T . On the other hand, the empirical risk of AdaBoost decreases with T . In
fact, as we demonstrate later, T can be used to decrease the approximation error
of L(B, T ). Therefore, the parameter T of AdaBoost enables us to control the bias-
complexity tradeoff.
the simple example, in which X = R and the base class is Decision Stumps,

To demonstrate how the expressive power of L(B, T ) increases with T , consider

HDS1 = {x (cid:29)→ sign(x − θ)· b : θ ∈ R,b ∈ {±1}}.

Note that in this one dimensional case, HDS1 is in fact equivalent to (nonhomoge-
nous) halfspaces on R.
Now, let H be the rather complex class (compared to halfspaces on the line) of
piece-wise constant functions. Let gr be a piece-wise constant function with at most
r pieces; that is, there exist thresholds −∞ = θ0 < θ1 < θ2 < ··· < θr = ∞ such that

gr(x) = r(cid:7)

i=1

αi 1[x∈(θi−1,θi ]] ∀i , αi ∈ {±1}.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

10.3 Linear Combinations of Base Hypotheses
Denote by Gr the class of all such piece-wise constant classiﬁers with at most r
pieces.
In the following we show that GT ⊆ L(HDS1, T ); namely, the class of halfspaces
over T decision stumps yields all the piece-wise constant classiﬁers with at most T
pieces.
Indeed, without loss of generality consider any g ∈ GT with αt = ( − 1)t. This
implies that if x is in the interval (θt−1, θt ], then g(x) = (− 1)t . For example:

109

Now, the function

(cid:27)
wt sign(x − θt−1)

,

(cid:26)

T(cid:7)

t=1

h(x) = sign

(10.5)

where w1 = 0.5 and for t > 1, wt = ( − 1)t, is in L(HDS1, T ) and is equal to g (see
Exercise 10.2).
From this example we obtain that L(HDS1, T ) can shatter any set of T + 1
instances in R; hence the VC-dimension of L(HDS1, T ) is at least T + 1. Therefore,
T is a parameter that can control the bias-complexity tradeoff: Enlarging T yields
a more expressive hypothesis class but on the other hand might increase the esti-
mation error. In the next subsection we formally upper bound the VC-dimension of
L(B, T ) for any base class B.

10.3.1 The VC-Dimension of L(B, T )
The following lemma tells us that the VC-dimension of L(B, T ) is upper bounded
by ˜O(VCdim(B) T ) (the ˜O notation ignores constants and logarithmic factors).
Lemma 10.3. Let B be a base class and let L(B, T ) be as deﬁned in Equation (10.4).
Assume that both T and VCdim(B) are at least 3. Then,

VCdim(L(B, T )) ≤ T (VCdim(B)+ 1)(3log(T (VCdim(B)+ 1))+ 2).

Proof. Denote d = VCdim(B). Let C = {x1, . . . , xm} be a set that is shattered by
L(B, T ). Each labeling of C by h ∈ L(B, T ) is obtained by ﬁrst choosing h1, . . . ,hT ∈
B and then applying a halfspace hypothesis over the vector (h1(x), . . . ,hT (x)). By
Sauer’s lemma, there are at most (em/d)d different dichotomies (i.e., labelings)
induced by B over C. Therefore, we need to choose T hypotheses, out of at most
(em/d)d different hypotheses. There are at most (em/d)dT ways to do it. Next,
for each such choice, we apply a linear predictor, which yields at most (em/T )T
dichotomies. Therefore, the overall number of dichotomies we can construct is

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

110

Boosting

upper bounded by

(em/d)dT (em/T )T ≤ m(d+1)T ,

where we used the assumption that both d and T are at least 3. Since we assume that
C is shattered, we must have that the preceding is at least 2m, which yields

Therefore,

2m ≤ m(d+1)T .

m ≤ log(m)

(d + 1)T
log(2)

.

Lemma A.1 in Appendix A tells us that a necessary condition for the preceding to
hold is that

m ≤ 2

(d + 1)T
log(2)

(d + 1)T
log(2)

log

≤ (d + 1)T (3log((d + 1)T )+ 2),

which concludes our proof.

In Exercise 10.4 we show that for some base classes, B,

VCdim(L(B, T )) ≥ (cid:6)(VCdim(B) T ).

it also holds that

10.4 ADABOOST FOR FACE RECOGNITION
We now turn to a base hypothesis that has been proposed by Viola and Jones for
the task of face recognition. In this task, the instance space is images, represented
as matrices of gray level values of pixels. To be concrete, let us take images of size
24× 24 pixels, and therefore our instance space is the set of real valued matrices of
size 24 × 24. The goal is to learn a classiﬁer, h : X → {±1}, that given an image as
input, should output whether the image is of a human face or not.
is a
decision stump hypothesis and g : R24,24 → R is a function that maps an image to a
scalar. Each function g is parameterized by
(cid:2) An axis aligned rectangle R. Since each image is of size 24×24, there are at most
t ∈ {A, B,C, D}. Each type corresponds to a mask, as depicted in
(cid:2) A type,

Each hypothesis in the base class is of the form h(x) = f (g(x)), where f

244 axis aligned rectangles.

Figure 10.1.

To calculate g we stretch the mask t to ﬁt the rectangle R and then calculate the
sum of the pixels (that is, sum of their gray level values) that lie within the outer
rectangles and subtract it from the sum of pixels in the inner rectangles.
Since the number of such functions g is at most 244· 4, we can implement a weak
learner for the base hypothesis class by ﬁrst calculating all the possible outputs of
g on each image, and then apply the weak learner of decision stumps described in
the previous subsection. It is possible to perform the ﬁrst step very efﬁciently by
a preprocessing step in which we calculate the integral image of each image in the
training set. See Exercise 10.5 for details.

In Figure 10.2 we depict the ﬁrst two features selected by AdaBoost when

running it with the base features proposed by Viola and Jones.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

10.6 Bibliographic Remarks

111

A

C

B

D

Figure 10.1. The four types of functions, g, used by the base hypotheses for face recogni-
tion. The value of g for type A or B is the difference between the sum of the pixels within
two rectangular regions. These regions have the same size and shape and are horizontally
or vertically adjacent. For type C, the value of g is the sum within two outside rectangles
subtracted from the sum in a center rectangle. For type D, we compute the difference
between diagonal pairs of rectangles.

Figure 10.2. The ﬁrst and second features selected by AdaBoost, as implemented by Viola
and Jones. The two features are shown in the top row and then overlaid on a typical train-
ing face in the bottom row. The ﬁrst feature measures the difference in intensity between
the region of the eyes and a region across the upper cheeks. The feature capitalizes on
the observation that the eye region is often darker than the cheeks. The second feature
compares the intensities in the eye regions to the intensity across the bridge of the nose.

10.5 SUMMARY
Boosting is a method for amplifying the accuracy of weak learners. In this chapter
we described the AdaBoost algorithm. We have shown that after T iterations of
AdaBoost, it returns a hypothesis from the class L(B, T ), obtained by composing a
linear classiﬁer on T hypotheses from a base class B. We have demonstrated how the
parameter T controls the tradeoff between approximation and estimation errors. In
the next chapter we will study how to tune parameters such as T, on the basis of the
data.

10.6 BIBLIOGRAPHIC REMARKS
As mentioned before, boosting stemmed from the theoretical question of whether
an efﬁcient weak learner can be “boosted” into an efﬁcient strong learner (Kearns
& Valiant 1988) and solved by Schapire (1990). The AdaBoost algorithm has been
proposed in Freund and Schapire (1995).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

112

Boosting

Boosting can be viewed from many perspectives. In the purely theoretical con-
text, AdaBoost can be interpreted as a negative result: If strong learning of a
hypothesis class is computationally hard, so is weak learning of this class. This neg-
ative result can be useful for showing hardness of agnostic PAC learning of a class
B based on hardness of PAC learning of some other class H, as long as H is weakly
learnable using B. For example, Klivans and Sherstov (2006) have shown that PAC
learning of the class of intersection of halfspaces is hard (even in the realizable case).
This hardness result can be used to show that agnostic PAC learning of a single half-
space is also computationally hard (Shalev-Shwartz, Shamir & Sridharan 2010). The
idea is to show that an agnostic PAC learner for a single halfspace can yield a weak
learner for the class of intersection of halfspaces, and since such a weak learner can
be boosted, we will obtain a strong learner for the class of intersection of halfspaces.
AdaBoost also shows an equivalence between the existence of a weak learner
and separability of the data using a linear classiﬁer over the predictions of base
hypotheses. This result is closely related to von Neumann’s minimax theorem (von
Neumann 1928), a fundamental result in game theory.

AdaBoost is also related to the concept of margin, which we will study later
on in Chapter 15. It can also be viewed as a forward greedy selection algorithm, a
topic that will be presented in Chapter 25. A recent book by Schapire and Freund
(2012) covers boosting from all points of view and gives easy access to the wealth of
research that this ﬁeld has produced.

10.7 EXERCISES
10.1 Boosting the Conﬁdence: Let A be an algorithm that guarantees the following:
There exist some constant δ0 ∈ (0,1) and a function mH : (0,1) → N such that
for every (cid:2) ∈ (0,1), if m ≥ mH((cid:2)) then for every distribution D it holds that with
probability of at least 1− δ0, LD(A(S)) ≤ minh∈H LD(h)+ (cid:2).
Suggest a procedure that relies on A and learns H in the usual agnostic PAC
(cid:10)

learning model and has a sample complexity of

(cid:9)

mH((cid:2), δ) ≤ k mH((cid:2))+

2log(4k/δ)

,

(cid:2)2

where

k = (cid:24)log(δ)/log(δ0)(cid:25).

Hint: Divide the data into k + 1 chunks, where each of the ﬁrst k chunks is of size
mH((cid:2)) examples. Train the ﬁrst k chunks using A. Argue that the probability that
for all of these chunks we have LD(A(S)) > minh∈H LD(h)+ (cid:2) is at most δk
≤ δ/2.
Finally, use the last chunk to choose from the k hypotheses that A generated from
the k chunks (by relying on Corollary 4.6).

0

10.2 Prove that the function h given in Equation (10.5) equals the piece-wise constant

function deﬁned according to the same thresholds as h.

10.3 We have informally argued that the AdaBoost algorithm uses the weighting mech-
anism to “force” the weak learner to focus on the problematic examples in the next
iteration. In this question we will ﬁnd some rigorous justiﬁcation for this argument.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011

10.7 Exercises

113

Show that the error of ht w.r.t. the distribution D(t+1) is exactly 1/2. That is, show
that for every t ∈ [T ]

m(cid:7)

i=1

D(t+1)

i

1[yi(cid:18)=ht (xi )] = 1/2.

10.4 In this exercise we discuss the VC-dimension of classes of the form L(B, T ). We
proved an upper bound of O(dT log(dT )), where d = VCdim(B). Here we wish to
prove an almost matching lower bound. However, that will not be the case for all
classes B.
for every class B and every number T ≥ 1, VCdim(B) ≤
1. Note that
VCdim(L(B, T )). Find a class B for which VCdim(B) = VCdim(L(B, T )) for
every T ≥ 1.
Hint: Take X to be a ﬁnite set.
VCdim(Bd) ≤ 5+ 2log (d).
Hints:
(cid:2) For the upper bound, rely on Exercise 10.11.
(cid:2) For the lower bound, assume d = 2k. Let A be a k × d matrix whose columns
are all the d binary vectors in {±1}k. The rows of A form a set of k vectors
in Rd . Show that this set is shattered by decision stumps over Rd.
3. Let T ≥ 1 be any integer. Prove that VCdim(L(Bd , T )) ≥ 0. 5 T log(d).

2. Let Bd be the class of decision stumps over Rd. Prove that log(d) ≤

Hint: Construct a set of T
the previous question, and the rows of the matrices 2A,3A,4A, . . . , T
that the resulting set is shattered by L(Bd , T ).

2 k instances by taking the rows of the matrix A from
2 A. Show

I(A), is the matrix B such that Bi , j =(cid:2)

10.5 Efﬁciently Calculating the Viola and Jones Features Using an Integral Image: Let
A be a 24× 24 matrix representing an image. The integral image of A, denoted by
(cid:2) Show that I(A) can be calculated from A in time linear in the size of A.
(cid:2) Show how every Viola and Jones feature can be calculated from I(A) in a con-
stant amount of time (that is, the runtime does not depend on the size of the
rectangle deﬁning the feature).

i(cid:3)≤i , j(cid:3)≤ j Ai , j.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:45:36, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.011


