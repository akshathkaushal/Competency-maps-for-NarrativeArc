6

The VC-Dimension

In the previous chapter, we decomposed the error of the ERMH rule into approx-
imation error and estimation error. The approximation error depends on the ﬁt
of our prior knowledge (as reﬂected by the choice of the hypothesis class H) to
the underlying unknown distribution. In contrast, the deﬁnition of PAC learn-
ability requires that the estimation error would be bounded uniformly over all
distributions.
Our current goal is to ﬁgure out which classes H are PAC learnable, and to
characterize exactly the sample complexity of learning a given hypothesis class. So
far we have seen that ﬁnite classes are learnable, but that the class of all functions
(over an inﬁnite size domain) is not. What makes one class learnable and the other
unlearnable? Can inﬁnite-size classes be learnable, and, if so, what determines their
sample complexity?

We begin the chapter by showing that inﬁnite classes can indeed be learn-
able, and thus, ﬁniteness of the hypothesis class is not a necessary condition for
learnability. We then present a remarkably crisp characterization of the family of
learnable classes in the setup of binary valued classiﬁcation with the zero-one loss.
This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey Chervo-
nenkis in 1970 and relies on a combinatorial notion called the Vapnik-Chervonenkis
dimension (VC-dimension). We formally deﬁne the VC-dimension, provide several
examples, and then state the fundamental theorem of statistical learning theory,
which integrates the concepts of learnability, VC-dimension, the ERM rule, and
uniform convergence.

6.1 INFINITE-SIZE CLASSES CAN BE LEARNABLE
In Chapter 4 we saw that ﬁnite classes are learnable, and in fact the sample complex-
ity of a hypothesis class is upper bounded by the log of its size. To show that the size
of the hypothesis class is not the right characterization of its sample complexity, we
ﬁrst present a simple example of an inﬁnite-size hypothesis class that is learnable.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

43

44

The VC-Dimension

Example 6.1. Let H be the set of threshold functions over the real line, namely,
H = {ha : a ∈ R}, where ha : R → {0,1} is a function such that ha(x) = 1[x <a]. To
remind the reader, 1[x <a] is 1 if x < a and 0 otherwise. Clearly, H is of inﬁnite size.
Nevertheless, the following lemma shows that H is learnable in the PAC model
using the ERM algorithm.
Lemma 6.1. Let H be the class of thresholds as deﬁned earlier. Then, H is PAC
learnable, using the ERM rule, with sample complexity of mH((cid:14), δ) ≤ (cid:24)log(2/δ)/(cid:14)(cid:25).
Proof. Let a(cid:12) be a threshold such that the hypothesis h(cid:12)(x) = 1[x <a(cid:12)] achieves
LD(h(cid:12)) = 0. Let Dx be the marginal distribution over the domain X and let a0 <
a(cid:12) < a1 be such that

[x ∈ (a0,a(cid:12))] = P
x∼Dx

[x ∈ (a(cid:12),a1)] = (cid:14).

(cid:2) mass

(cid:2) mass

a*

a1

P
x∼Dx

a0

(If Dx(−∞,a(cid:12)) ≤ (cid:14) we set a0 = −∞ and similarly for a1). Given a training set S,
let b0 = max{x : (x ,1) ∈ S} and b1 = min{x : (x ,0) ∈ S} (if no example in S is positive
we set b0 = −∞ and if no example in S is negative we set b1 = ∞). Let bS be a
threshold corresponding to an ERM hypothesis, h S, which implies that bS ∈ (b0,b1).
Therefore, a sufﬁcient condition for LD(h S) ≤ (cid:14) is that both b0 ≥ a0 and b1 ≤ a1. In
other words,

[LD(h S) > (cid:14)] ≤ P
S∼Dm

P

S∼Dm

[b0 < a0 ∨ b1 > a1],

and using the union bound we can bound the preceding by
[b0 < a0]+ P
S∼Dm

[LD(h S) > (cid:14)] ≤ P
S∼Dm

P
S∼Dm

[b1 > a1].

(6.1)

The event b0 < a0 happens if and only if all examples in S are not in the interval
(a0,a

), whose probability mass is deﬁned to be (cid:14), namely,

∗

[b0 < a0] = P
S∼Dm

P

S∼Dm

[∀(x , y) ∈ S, x (cid:18)∈ (a0,a(cid:12))] = (1− (cid:14))m ≤ e

−(cid:14) m.

Since we assume m > log(2/δ)/(cid:14) it follows that the equation is at most δ/2. In the
same way it is easy to see that PS∼Dm [b1 > a1] ≤ δ/2. Combining with Equation (6.1)
we conclude our proof.

6.2 THE VC-DIMENSION
We see, therefore, that while ﬁniteness of H is a sufﬁcient condition for learnability,
it is not a necessary condition. As we will show, a property called the VC-dimension
of a hypothesis class gives the correct characterization of its learnability. To moti-
vate the deﬁnition of the VC-dimension, let us recall the No-Free-Lunch theorem
(Theorem 5.1) and its proof. There, we have shown that without restricting the
hypothesis class, for any learning algorithm, an adversary can construct a distri-
bution for which the learning algorithm will perform poorly, while there is another

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.2 The VC-Dimension

45

learning algorithm that will succeed on the same distribution. To do so, the adver-
sary used a ﬁnite set C ⊂ X and considered a family of distributions that are
concentrated on elements of C. Each distribution was derived from a “true” tar-
get function from C to {0,1}. To make any algorithm fail, the adversary used the
power of choosing a target function from the set of all possible functions from C to
{0,1}.
When considering PAC learnability of a hypothesis class H, the adversary is
restricted to constructing distributions for which some hypothesis h ∈ H achieves a
zero risk. Since we are considering distributions that are concentrated on elements
of C, we should study how H behaves on C, which leads to the following deﬁnition.
Deﬁnition 6.2 (Restriction of H to C). Let H be a class of functions from X to {0,1}
and let C = {c1, . . . ,cm} ⊂ X . The restriction of H to C is the set of functions from C
to {0,1} that can be derived from H. That is,

HC = {(h(c1), . . . ,h(cm)) : h ∈ H},

where we represent each function from C to {0,1} as a vector in {0,1}|C|

.

If the restriction of H to C is the set of all functions from C to {0,1}, then we say

that H shatters the set C. Formally:
Deﬁnition 6.3 (Shattering). A hypothesis class H shatters a ﬁnite set C ⊂ X if the
restriction of H to C is the set of all functions from C to {0,1}. That is, |HC| = 2
|C|
.
Example 6.2. Let H be the class of threshold functions over R. Take a set C = {c1}.
Now, if we take a = c1 + 1, then we have ha(c1) = 1, and if we take a = c1 − 1, then
we have ha(c1) = 0. Therefore, HC is the set of all functions from C to {0,1}, and H
shatters C. Now take a set C = {c1,c2}, where c1 ≤ c2. No h ∈ H can account for the
labeling (0,1), because any threshold that assigns the label 0 to c1 must assign the
label 0 to c2 as well. Therefore not all functions from C to {0,1} are included in HC;
hence C is not shattered by H.

Getting back to the construction of an adversarial distribution as in the proof
of the No-Free-Lunch theorem (Theorem 5.1), we see that whenever some set C is
shattered by H, the adversary is not restricted by H, as they can construct a distri-
bution over C based on any target function from C to {0,1}, while still maintaining
the realizability assumption. This immediately yields:
Corollary 6.4. Let H be a hypothesis class of functions from X to {0,1}. Let m be a
training set size. Assume that there exists a set C ⊂ X of size 2m that is shattered by
H. Then, for any learning algorithm, A, there exist a distribution D over X × {0,1}
and a predictor h ∈ H such that LD(h) = 0 but with probability of at least 1/7 over the
choice of S ∼ Dm we have that LD(A(S)) ≥ 1/8.

Corollary 6.4 tells us that if H shatters some set C of size 2m then we cannot learn
H using m examples. Intuitively, if a set C is shattered by H, and we receive a sample
containing half the instances of C, the labels of these instances give us no informa-
tion about the labels of the rest of the instances in C – every possible labeling of the
rest of the instances can be explained by some hypothesis in H. Philosophically,

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

46

The VC-Dimension

If someone can explain every phenomenon, his explanations are worthless.

This leads us directly to the deﬁnition of the VC dimension.

Deﬁnition 6.5 (VC-dimension). The VC-dimension of a hypothesis class H,
denoted VCdim(H), is the maximal size of a set C ⊂X that can be shattered by H. If
H can shatter sets of arbitrarily large size we say that H has inﬁnite VC-dimension.

A direct consequence of Corollary 6.4 is therefore:

Theorem 6.6. Let H be a class of inﬁnite VC-dimension. Then, H is not PAC
learnable.
Proof. Since H has an inﬁnite VC-dimension, for any training set size m, there exists
a shattered set of size 2m, and the claim follows by Corollary 6.4.

We shall see later in this chapter that the converse is also true: A ﬁnite VC-
dimension guarantees learnability. Hence, the VC-dimension characterizes PAC
learnability. But before delving into more theory, we ﬁrst show several examples.

6.3 EXAMPLES
In this section we calculate the VC-dimension of several hypothesis classes. To show
that VCdim(H) = d we need to show that

1. There exists a set C of size d that is shattered by H.
2. Every set C of size d + 1 is not shattered by H.

6.3.1 Threshold Functions
Let H be the class of threshold functions over R. Recall Example 6.2, where we have
shown that for an arbitrary set C = {c1}, H shatters C; therefore VCdim(H) ≥ 1. We
have also shown that for an arbitrary set C = {c1,c2} where c1 ≤ c2, H does not
shatter C. We therefore conclude that VCdim(H) = 1.

6.3.2 Intervals
Let H be the class of intervals over R, namely, H ={ha,b : a,b∈ R,a < b}, where ha,b :
R → {0,1} is a function such that ha,b(x) = 1[x∈(a,b)]. Take the set C = {1,2}. Then, H
shatters C (make sure you understand why) and therefore VCdim(H)≥ 2. Now take
an arbitrary set C ={c1,c2,c3} and assume without loss of generality that c1 ≤ c2 ≤ c3.
Then, the labeling (1,0,1) cannot be obtained by an interval and therefore H does
not shatter C. We therefore conclude that VCdim(H) = 2.

6.3.3 Axis Aligned Rectangles
Let H be the class of axis aligned rectangles, formally:

H = {h(a1,a2,b1,b2) : a1 ≤ a2 and b1 ≤ b2}

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.3 Examples

47

c4

c1

c5

c3

c2

Figure 6.1. Left: 4 points that are shattered by axis aligned rectangles. Right: Any axis
aligned rectangle cannot label c5 by 0 and the rest of the points by 1.

where

h(a1,a2,b1,b2)(x1, x2) =

(cid:5)

1 if a1 ≤ x1 ≤ a2
0 otherwise

and b1 ≤ x2 ≤ b2

(6.2)

We shall show in the following that VCdim(H) = 4. To prove this we need to ﬁnd
a set of 4 points that are shattered by H, and show that no set of 5 points can be
shattered by H. Finding a set of 4 points that are shattered is easy (see Figure 6.1).
Now, consider any set C ⊂ R2 of 5 points. In C, take a leftmost point (whose ﬁrst
coordinate is the smallest in C), a rightmost point (ﬁrst coordinate is the largest), a
lowest point (second coordinate is the smallest), and a highest point (second coor-
dinate is the largest). Without loss of generality, denote C = {c1, . . . ,c5} and let c5
be the point that was not selected. Now, deﬁne the labeling (1,1,1,1,0). It is impos-
sible to obtain this labeling by an axis aligned rectangle. Indeed, such a rectangle
must contain c1, . . . ,c4; but in this case the rectangle contains c5 as well, because
its coordinates are within the intervals deﬁned by the selected points. So, C is not
shattered by H , and therefore VCdim(H ) = 4.

|C|

6.3.4 Finite Classes
Let H be a ﬁnite class. Then, clearly, for any set C we have |HC| ≤ |H| and thus
. This implies that VCdim(H) ≤ log2 (|H|). This
C cannot be shattered if |H| < 2
shows that the PAC learnability of ﬁnite classes follows from the more general state-
ment of PAC learnability of classes with ﬁnite VC-dimension, which we shall see in
the next section. Note, however, that the VC-dimension of a ﬁnite class H can be
signiﬁcantly smaller than log2 (|H|). For example, let X = {1, . . . ,k}, for some inte-
ger k, and consider the class of threshold functions (as deﬁned in Example 6.2).
Then, |H| = k but VCdim(H) = 1. Since k can be arbitrarily large, the gap between
log2 (|H|) and VCdim(H) can be arbitrarily large.

6.3.5 VC-Dimension and the Number of Parameters
In the previous examples, the VC-dimension happened to equal the number of
parameters deﬁning the hypothesis class. While this is often the case, it is not
always true. Consider, for example, the domain X = R, and the hypothesis class
H = {hθ : θ ∈ R} where hθ : X → {0,1} is deﬁned by hθ (x) = (cid:24)0.5 sin (θ x)(cid:25). It is pos-
sible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d points that
are shattered by H (see Exercise 6.8).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

48

The VC-Dimension

6.4 THE FUNDAMENTAL THEOREM OF PAC LEARNING
We have already shown that a class of inﬁnite VC-dimension is not learnable. The
converse statement is also true, leading to the fundamental theorem of statistical
learning theory:
Theorem 6.7 (The Fundamental Theorem of Statistical Learning). Let H be a
hypothesis class of functions from a domain X to {0,1} and let the loss function be the
0−1 loss. Then, the following are equivalent:

1. H has the uniform convergence property.
2. Any ERM rule is a successful agnostic PAC learner for H.
3. H is agnostic PAC learnable.
4. H is PAC learnable.
5. Any ERM rule is a successful PAC learner for H.
6. H has a ﬁnite VC-dimension.

The proof of the theorem is given in the next section.
Not only does the VC-dimension characterize PAC learnability; it even deter-

mines the sample complexity.

Theorem 6.8 (The Fundamental Theorem of Statistical Learning – Quantitative
Version). Let H be a hypothesis class of functions from a domain X to {0,1} and let
the loss function be the 0−1 loss. Assume that VCdim(H) = d < ∞. Then, there are
absolute constants C1,C2 such that

1. H has the uniform convergence property with sample complexity

d + log(1/δ)

(cid:14)2

C1

≤ mUCH ((cid:14), δ) ≤ C2

d + log(1/δ)

(cid:14)2

2. H is agnostic PAC learnable with sample complexity

d + log(1/δ)

(cid:14)2

C1

≤ mH((cid:14), δ) ≤ C2

d + log(1/δ)

(cid:14)2

3. H is PAC learnable with sample complexity
≤ mH((cid:14), δ) ≤ C2

d + log(1/δ)

C1

(cid:14)

d log(1/(cid:14))+ log(1/δ)

(cid:14)

The proof of this theorem is given in Chapter 28.

Remark 6.3. We stated the fundamental theorem for binary classiﬁcation tasks. A
similar result holds for some other learning problems such as regression with the
absolute loss or the squared loss. However, the theorem does not hold for all learn-
ing tasks. In particular, learnability is sometimes possible even though the uniform
convergence property does not hold (we will see an example in Chapter 13, Exercise
6.2). Furthermore, in some situations, the ERM rule fails but learnability is possible
with other learning rules.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.5 Proof of Theorem 6.7

49

6.5 PROOF OF THEOREM 6.7
We have already seen that 1 → 2 in Chapter 4. The implications 2 → 3 and 3 → 4
are trivial and so is 2 → 5. The implications 4 → 6 and 5 → 6 follow from the No-
Free-Lunch theorem. The difﬁcult part is to show that 6 → 1. The proof is based on
two main claims:
(cid:2) If VCdim(H) = d, then even though H might be inﬁnite, when restricting it to
a ﬁnite set C ⊂ X , its “effective” size, |HC|, is only O(|C|d). That is, the size of
HC grows polynomially rather than exponentially with |C|. This claim is often
referred to as Sauer’s lemma, but it has also been stated and proved indepen-
dently by Shelah and by Perles. The formal statement is given in Section 6.5.1
later.
(cid:2) In Section 4 we have shown that ﬁnite hypothesis classes enjoy the uniform con-
vergence property. In Section 6.5.2 later we generalize this result and show
that uniform convergence holds whenever the hypothesis class has a “small
effective size.” By “small effective size” we mean classes for which |HC| grows
polynomially with |C|.

6.5.1 Sauer’s Lemma and the Growth Function
We deﬁned the notion of shattering, by considering the restriction of H to a ﬁnite
set of instances. The growth function measures the maximal “effective” size of H on
a set of m examples. Formally:
Deﬁnition 6.9 (Growth Function). Let H be a hypothesis class. Then the growth
function of H, denoted τH : N → N, is deﬁned as
τH(m) = max

|HC|.

C⊂X :|C|=m

In words, τH (m) is the number of different functions from a set C of size m to {0,1}
that can be obtained by restricting H to C.

Obviously, if VCdim(H) = d then for any m ≤ d we have τH(m) = 2m. In such
cases, H induces all possible functions from C to {0,1}. The following beautiful
lemma, proposed independently by Sauer, Shelah, and Perles, shows that when m
becomes larger than the VC-dimension, the growth function increases polynomially
rather than exponentially with m.
(cid:4)
Lemma 6.10 (Sauer-Shelah-Perles). Let H be a hypothesis class with VCdim(H) ≤
. In particular, if m > d + 1 then τH(m) ≤

d < ∞. Then, for all m, τH(m) ≤(cid:2)

(cid:3)

d
i=0

m
i

(em/d)d.

Proof of Sauer’s Lemma*
To prove the lemma it sufﬁces to prove the following stronger claim: For any C =
{c1, . . . ,cm} we have

∀H,

|HC| ≤ |{B ⊆ C : H shatters B}|.

(6.3)

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

50

The VC-Dimension

The reason why Equation (6.3) is sufﬁcient to prove the lemma is that if
VCdim(H)≤ d then no set whose size is larger than d is shattered by H and therefore

|{B ⊆ C : H shatters B}| ≤ d(cid:7)

(cid:20)

(cid:21)

.

m
i

i=0

When m > d + 1 the right-hand side of the preceding is at most (em/d)d (see
Lemma A.5 in Appendix A).
We are left with proving Equation (6.3) and we do it using an inductive argu-
ment. For m = 1, no matter what H is, either both sides of Equation (6.3) equal
1 or both sides equal 2 (the empty set is always considered to be shattered by H).
Assume Equation (6.3) holds for sets of size k < m and let us prove it for sets of size
m. Fix H and C = {c1, . . . ,cm}. Denote C
(cid:3) = {c2, . . . ,cm} and in addition, deﬁne the
following two sets:

Y0 = {(y2, . . . , ym) : (0, y2, . . . , ym) ∈ HC ∨ (1, y2, . . . , ym) ∈ HC},

and

Y1 = {(y2, . . . , ym) : (0, y2, . . . , ym) ∈ HC ∧ (1, y2, . . . , ym) ∈ HC}.

It is easy to verify that |HC| = |Y0| + |Y1|. Additionally, since Y0 = HC(cid:3), using the
induction assumption (applied on H and C

) we have that

(cid:3)

(cid:3)

: H shatters B}| = |{B ⊆ C : c1 (cid:18)∈ B ∧H shatters B}|.

|Y0| = |HC(cid:3)| ≤ |{B ⊆ C
Next, deﬁne H(cid:3) ⊆ H to be

H(cid:3) = {h ∈ H : ∃h

(cid:3) ∈ H s.t. (1− h
= (h(c1),h(c2), . . . ,h(cm)},

(cid:3)

(c1),h

(cid:3)

(c2), . . . ,h

(cid:3)

(cm))

contains pairs of hypotheses that agree on C

namely, H(cid:3)
this deﬁnition, it is clear that if H(cid:3)
B ∪ {c1} and vice versa. Combining this with the fact that Y1 = H(cid:3)
inductive assumption (now applied on H(cid:3)

shatters a set B ⊆ C

) we obtain that

and differ on c1. Using
then it also shatters the set
C(cid:3) and using the

and C

(cid:3)

(cid:3)

(cid:3)

|Y1| = |H(cid:3)

C(cid:3)| ≤ |{B ⊆ C

: H(cid:3)
= |{B ⊆ C : c1 ∈ B ∧H(cid:3)

(cid:3)

shatters B}| = |{B ⊆ C
shatters B ∪{c1}}|
shatters B}| ≤ |{B ⊆ C : c1 ∈ B ∧H shatters B}|.

: H(cid:3)

(cid:3)

Overall, we have shown that

|HC| = |Y0|+|Y1|

≤ |{B ⊆ C : c1 (cid:18)∈ B ∧H shatters B}|+|{B ⊆ C : c1 ∈ B ∧H shatters B}|
= |{B ⊆ C : H shatters B}|,

which concludes our proof.

6.5.2 Uniform Convergence for Classes of Small Effective Size
In this section we prove that if H has small effective size then it enjoys the uniform
convergence property. Formally,

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.5 Proof of Theorem 6.7

51

Theorem 6.11. Let H be a class and let τH be its growth function. Then, for every D
and every δ ∈ (0,1), with probability of at least 1 − δ over the choice of S ∼ Dm we
have

|LD(h)− L S(h)| ≤ 4+(cid:22)

√
log(τH(2m))
2m
δ

.

Before proving the theorem, let us ﬁrst conclude the proof of Theorem 6.7.

Proof of Theorem 6.7. It sufﬁces to prove that if the VC-dimension is ﬁnite then the
uniform convergence property holds. We will prove that

(cid:20)

(cid:21)

mUCH ((cid:14), δ) ≤ 4

16d
(δ(cid:14))2 log

16d
(δ(cid:14))2

+ 16 d log(2e/d)

(δ(cid:14))2

.

From Sauer’s lemma we have that for m > d, τH(2m) ≤ (2em/d)d. Combining this
with Theorem 6.11 we obtain that with probability of at least 1− δ,

|L S(h)− LD(h)| ≤ 4+(cid:22)
√
d log(2em/d)
(cid:22)
2m
δ
(cid:23)
d log(2em/d) ≥ 4; hence,

|L S(h)− LD(h)| ≤ 1

2d log(2em/d)

.

.

For simplicity assume that

m
To ensure that the preceding is at most (cid:14) we need that

δ

m ≥ 2d log(m)

(δ(cid:14))2

+ 2 d log(2e/d)

(δ(cid:14))2

.

Standard algebraic manipulations (see Lemma A.2 in Appendix A) show that a
sufﬁcient condition for the preceding to hold is that

(cid:20)

(cid:21)

m ≥ 4

2d
(δ(cid:14))2 log

2d
(δ(cid:14))2

+ 4 d log(2e/d)

(δ(cid:14))2

.

Remark 6.4. The upper bound on mUCH we derived in the proof Theorem 6.7 is not
the tightest possible. A tighter analysis that yields the bounds given in Theorem 6.8
can be found in Chapter 28.

Proof of Theorem 6.11*
We will start by showing that

(cid:24)

(cid:25)

≤ 4+(cid:22)

|LD(h)− L S(h)|

sup
h∈H

E
S∼Dm

(6.4)
Since the random variable suph∈H|LD(h) − L S(h)| is nonnegative, the proof of
the theorem follows directly from the preceding using Markov’s inequality (see
Section B.1).

.

√
log(τH(2m))
2m

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

52

The VC-Dimension

To bound the left-hand side of Equation (6.4) we ﬁrst note that for every h ∈ H,
(cid:3)
m is an additional i.i.d.

we can rewrite LD(h) = ES(cid:3)∼Dm [L S(cid:3)(h)], where S
(cid:24)
sample. Therefore,

(cid:3) = z

, . . . , z

(cid:24)

(cid:25)

(cid:25)

(cid:3)
1

E
S∼Dm

sup
h∈H

|LD(h)− L S(h)|

= E
S∼Dm

sup
h∈H

L S(cid:3)(h)− L S(h)

.

(cid:14)(cid:14)(cid:14)(cid:14) E

S(cid:3)∼Dm

(cid:14)(cid:14)(cid:14)(cid:14)

A generalization of the triangle inequality yields

(cid:14)(cid:14)(cid:14)(cid:14) E

S(cid:3)∼Dm

(cid:14)(cid:14)(cid:14)(cid:14) ≤ E

S(cid:3)∼Dm

[L S(cid:3)(h)− L S(h)]

|L S(cid:3)(h)− L S(h)|,

and the fact that supermum of expectation is smaller than expectation of supremum
yields

sup
h∈H

E

S(cid:3)∼Dm

|L S(cid:3)(h)− L S(h)| ≤ E
S(cid:3)∼Dm

sup
h∈H

|L S(cid:3)(h)− L S(h)|.

Formally, the previous two inequalities follow from Jensen’s inequality. Combining
all we obtain

(cid:24)

|LD(h)− L S(h)|

E
S∼Dm

sup
h∈H

(cid:24)
(cid:13)

|L S(cid:3)(h)− L S(h)|

≤ E

S,S(cid:3)∼Dm

= E

S,S(cid:3)∼Dm

sup
h∈H

1
m

sup
h∈H

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)

i=1

((cid:2)(h, z

i )− (cid:2)(h, zi ))
(cid:3)

(cid:15)

.

(6.5)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)

(cid:3)
1

, . . . , z

(cid:3) = z
(cid:3)
i . If we do it, instead of the term ((cid:2)(h, z

The expectation on the right-hand side is over a choice of two i.i.d. samples S =
(cid:3)
m. Since all of these 2m vectors are chosen i.i.d., nothing
z1, . . . , zm and S
will change if we replace the name of the random vector zi with the name of the
i )−(cid:2)(h, zi )) in Equation (6.5)
(cid:3)
random vector z
i ) − (cid:2)(h, zi )). It follows that for every σ ∈ {±1}m we
we will have the term −((cid:2)(h, z
(cid:3)
(cid:13)
have that Equation (6.5) equals

(cid:15)

E

S,S(cid:3)∼Dm

1
m

sup
h∈H

σi ((cid:2)(h, z

i )− (cid:2)(h, zi ))
(cid:3)

Since this holds for every σ ∈ {±1}m, it also holds if we sample each component of σ
uniformly at random from the uniform distribution over {±1}, denoted U±. Hence,
Equation (6.5) also equals

E
σ∼U m±

E

S,S(cid:3)∼Dm

1
m

sup
h∈H

σi ((cid:2)(h, z

i )− (cid:2)(h, zi ))
(cid:3)

and by the linearity of expectation it also equals

E

S,S(cid:3)∼Dm

E
σ∼U m±

1
m

sup
h∈H

σi ((cid:2)(h, z

i )− (cid:2)(h, zi ))
(cid:3)

(cid:25)

(cid:13)

(cid:13)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)

i=1

i=1

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)
(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)

i=1

(cid:25)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)

(cid:15)

,

.

(cid:15)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)
(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.7 Bibliographic Remarks

53

(cid:3)

. Then, we can

Next, ﬁx S and S
take the supremum only over h ∈ HC. Therefore,

, and let C be the instances appearing in S and S

(cid:3)

(cid:13)

E
σ∼U m±

sup
h∈H

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)

i=1

1
m

(cid:13)

σi ((cid:2)(h, z

i )− (cid:2)(h, zi ))
(cid:3)

(cid:15)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)

(cid:15)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)

= E
σ∼U m±

i )− (cid:2)(h, zi ))
(cid:3)
1
max
h∈HC
m
Fix some h ∈ HC and denote θh = 1
i )− (cid:2)(h, zi )). Since E[θh] = 0 and
(cid:3)
θh is an average of independent variables, each of which takes values in [− 1,1], we
have by Hoeffding’s inequality that for every ρ > 0,

i=1
m
i=1

σi ((cid:2)(h, z

σi ((cid:2)(h, z

m

.

P[|θh| > ρ] ≤ 2 exp

(cid:17)

.

(cid:16)

−2 m ρ2
(cid:16)

(cid:17)

Applying the union bound over h ∈ HC, we obtain that for any ρ > 0,

|θh| > ρ

≤ 2|HC| exp

−2 m ρ2

.

Finally, Lemma A.4 in Appendix A tells us that the preceding implies

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) m(cid:7)
(cid:2)

(cid:25)

(cid:25)

(cid:24)

P

max
h∈HC

(cid:24)

(cid:24)

|θh|

E

max
h∈HC

≤ 4+(cid:22)
(cid:25)

log(|HC|)
√
2m

.

≤ 4+(cid:22)

Combining all with the deﬁnition of τH, we have shown that

|LD(h)− L S(h)|

E
S∼Dm

sup
h∈H

log(τH(2m))
√
2m

.

6.6 SUMMARY
The fundamental theorem of learning theory characterizes PAC learnability of
classes of binary classiﬁers using VC-dimension. The VC-dimension of a class is a
combinatorial property that denotes the maximal sample size that can be shattered
by the class. The fundamental theorem states that a class is PAC learnable if and
only if its VC-dimension is ﬁnite and speciﬁes the sample complexity required for
PAC learning. The theorem also shows that if a problem is at all learnable, then
uniform convergence holds and therefore the problem is learnable using the ERM
rule.

6.7 BIBLIOGRAPHIC REMARKS
The deﬁnition of VC-dimension and its relation to learnability and to uniform con-
vergence is due to the seminal work of Vapnik and Chervonenkis (1971). The
relation to the deﬁnition of PAC learnability is due to Blumer, Ehrenfeucht,
Haussler, and Warmuth (1989).

Several generalizations of the VC-dimension have been proposed. For example,
the fat-shattering dimension characterizes learnability of some regression prob-
lems (Kearns, Schapire & Sellie 1994; Alon, Ben-David, Cesa-Bianchi & Haussler

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

54

The VC-Dimension

1997; Bartlett, Long & Williamson 1994; Anthony & Bartlet 1999), and the
Natarajan dimension characterizes learnability of some multiclass learning prob-
lems (Natarajan 1989). However,
in general, there is no equivalence between
learnability and uniform convergence. See (Shalev-Shwartz, Shamir, Srebro &
Sridharan 2010; Daniely, Sabato, Ben-David & Shalev-Shwartz 2011).

Sauer’s lemma has been proved by Sauer in response to a problem of Erdos
(Sauer 1972). Shelah (with Perles) proved it as a useful lemma for Shelah’s theory
of stable models (Shelah 1972). Gil Kalai tells1 us that at some later time, Benjy
Weiss asked Perles about such a result in the context of ergodic theory, and Perles,
who forgot that he had proved it once, proved it again. Vapnik and Chervonenkis
proved the lemma in the context of statistical learning theory.

) ≤ VCdim(H).

esis classes if H(cid:3) ⊆ H then VCdim(H(cid:3)

6.8 EXERCISES
6.1 Show the following monotonicity property of VC-dimension: For every two hypoth-
6.2 Given some ﬁnite domain set, X , and a number k ≤|X|, ﬁgure out the VC-dimension
: |{x : h(x) = 1}| = k}: that is, the set of all functions that assign

of each of the following classes (and prove your claims):
1. HX
=k
the value 1 to exactly k elements of X .
2. Hat−most−k = {h ∈ {0,1}X
: |{x : h(x) = 1}| ≤ k or|{x : h(x) = 0}| ≤ k}.
function h I as follows. On a binary vector x = (x1, x2, . . . , xn) ∈ {0,1}n,

6.3 Let X be the Boolean hypercube {0,1}n. For a set I ⊆ {1,2, . . . , n} we deﬁne a parity

= {h ∈ {0,1}X

(cid:26)(cid:7)

(cid:27)

h I (x) =

xi

mod 2 .

i∈I

(That is, h I computes parity of bits in I.) What is the VC-dimension of the class of
all such parity functions, Hn-parity = {h I
6.4 We proved Sauer’s lemma by proving that for every class H of ﬁnite VC-dimension

: I ⊆ {1,2, . . . , n}}?

d, and every subset A of the domain,

|HA| ≤ |{B ⊆ A : H shatters B}| ≤ d(cid:7)

(cid:20)|A|
(cid:21)

.

i=0

i

rec) = 2d.

Show that there are cases in which the previous two inequalities are strict (namely,
the ≤ can be replaced by <) and cases in which they can be replaced by equalities.
Demonstrate all four combinations of = and <.
6.5 VC-dimension of axis aligned rectangles in Rd: Let Hd
rec be the class of axis aligned
rec)= 4. Prove that in general,
rectangles in Rd. We have already seen that VCdim(H2
VCdim(Hd
con be the class of Boolean conjunc-
tions over the variables x1, . . . , xd (d ≥ 2). We already know that this class is ﬁnite
and thus (agnostic) PAC learnable. In this question we calculate VCdim(Hd
1. Show that |Hd
2. Conclude that VCdim(H) ≤ d log3.
3. Show that Hd

6.6 VC-dimension of Boolean conjunctions: Let Hd

con shatters the set of unit vectors {ei : i ≤ d}.

| ≤ 3d + 1.

con).

con

1 http://gilkalai.wordpress.com/2008/09/28/extremal-combinatorics-iii-some-basic-theorems

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.8 Exercises

55

4. (**) Show that VCdim(Hd

con) ≤ d.

Hint: Assume by contradiction that there exists a set C = {c1, . . . , cd+1} that is
shattered by Hd

con that satisfy

con. Let h1, . . . , hd+1 be hypotheses in Hd
0 i = j
1 otherwise

∀i , j ∈ [d + 1], hi(c j) =

(cid:5)

−

5. Consider the class Hd

For each i ∈ [d + 1], hi (or more accurately, the conjunction that corresponds to
hi) contains some literal (cid:2)i which is false on ci and true on c j for each j (cid:18)= i. Use
the Pigeonhole principle to show that there must be a pair i < j ≤ d + 1 such
that (cid:2)i and (cid:2) j use the same xk and use that fact to derive a contradiction to the
requirements from the conjunctions hi , h j.
mcon of monotone Boolean conjunctions over {0,1}d . Mono-
tonicity here means that the conjunctions do not contain negations. As in Hd
con,
the empty conjunction is interpreted as the all-positive hypothesis. We augment
Hd
6.7 We have shown that for a ﬁnite hypothesis class H, VCdim(H) ≤ (cid:27)log(|H|)(cid:28). How-
ever, this is just an upper bound. The VC-dimension of a class can be much lower
than that:
1. Find an example of a class H of functions over the real interval X = [0,1] such
2. Give an example of a ﬁnite hypothesis class H over the domain X = [0,1], where

that H is inﬁnite while VCdim(H) = 1.
VCdim(H) = (cid:27)log2 (|H|)(cid:28).

mcon with the all-negative hypothesis h

. Show that VCdim(Hd

mcon) = d.

6.8 (*) It is often the case that the VC-dimension of a hypothesis class equals (or can
be bounded above by) the number of parameters one needs to set in order to deﬁne
each hypothesis in the class. For instance, if H is the class of axis aligned rectangles in
Rd, then VCdim(H)= 2d, which is equal to the number of parameters used to deﬁne
a rectangle in Rd. Here is an example that shows that this is not always the case.
We will see that a hypothesis class might be very complex and even not learnable,
although it has a small number of parameters.

Consider the domain X = R, and the hypothesis class
H = {x (cid:29)→ (cid:24)sin(θ x)(cid:25) : θ ∈ R}
(here, we take (cid:24)−1(cid:25) = 0). Prove that VCdim(H) = ∞.
Hint: There is more than one way to prove the required result. One option is by
applying the following lemma: If 0. x1x2x3 . . ., is the binary expansion of x ∈ (0,1),
then for any natural number m, (cid:24)sin(2m π x)(cid:25) = (1 − xm), provided that ∃k ≥ m s.t.
xk = 1.
H = {ha,b,s : a ≤ b, s ∈ {−1,1}} where
ha,b,s(x) =

6.9 Let H be the class of signed intervals, that is,

if x ∈ [a, b]
if x /∈ [a, b]

(cid:5)
s
−s

Calculate VCdim(H).
1. Prove that if VCdim(H) ≥ d, for any d, then for some probability distribution D

6.10 Let H be a class of functions from X to {0,1}.
over X ×{0,1}, for every sample size, m,

[LD(A(S))]≥ min

h∈H LD(h)+ d − m

2d

E
S∼Dm

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

56

The VC-Dimension

Hint: Use Exercise 6.3 in Chapter 5.
the implication 3 → 6 in Theorem 6.7.)

2. Prove that for every H that is PAC learnable, VCdim(H) < ∞. (Note that this is
6.11 VC of union: Let H1, . . . ,Hr be hypothesis classes over some ﬁxed domain set X .

Let d = maxi VCdim(Hi) and assume for simplicity that d ≥ 3.
1. Prove that

(cid:3)∪r

Hi

i=1

(cid:4) ≤ 4d log(2d)+ 2log(r).

VCdim

Hint: Take a set of k examples and assume that they are shattered by the union
class. Therefore, the union class can produce all 2k possible labelings on these
examples. Use Sauer’s lemma to show that the union class cannot produce more
than rkd labelings. Therefore, 2k <rkd. Now use Lemma A.2.

2. (*) Prove that for r = 2 it holds that

VCdim (H1 ∪H2) ≤ 2d + 1.

6.12 Dudley classes: In this question we discuss an algebraic framework for deﬁning con-
cept classes over Rn and show a connection between the VC dimension of such
classes and their algebraic properties. Given a function f : Rn → R we deﬁne the
corresponding function, POS( f )(x) = 1[ f (x)>0]. For a class F of real valued func-
tions we deﬁne a corresponding class of functions POS(F) = {POS( f ) : f ∈ F}. We
say that a family, F, of real valued functions is linearly closed if for all f , g ∈ F and
r ∈ R, ( f +rg)∈ F (where addition and scalar multiplication of functions are deﬁned
point wise, namely, for all x ∈ Rn, ( f + rg)(x) = f (x)+ rg(x)). Note that if a family
of functions is linearly closed then we can view it as a vector space over the reals.
For a function g : Rn → R and a family of functions F, let F + g def= { f + g : f ∈ F}.
Hypothesis classes that have a representation as POS(F + g) for some vector space
of functions F and some function g are called Dudley classes.
1. Show that for every g : Rn → R and every vector space of functions F as deﬁned
earlier, VCdim(POS(F + g)) = VCdim(POS(F)).
2. (**) For every linearly closed family of real valued functions F, the VC-
dimension of the corresponding class POS(F) equals the linear dimension of
F (as a vector space). Hint: Let f1, . . . , fd be a basis for the vector space F. Con-
sider the mapping x (cid:29)→ ( f1(x), . . . , fd(x)) (from Rn to Rd). Note that this mapping
induces a matching between functions over Rn of the form POS( f ) and homo-
geneous linear halfspaces in Rd (the VC-dimension of the class of homogeneous
linear halfspaces is analyzed in Chapter 9).

3. Show that each of the following classes can be represented as a Dudley class:

1. The class H Sn of halfspaces over Rn (see Chapter 9).
2. The class H H Sn of all homogeneous halfspaces over Rn (see Chapter 9).
3. The class Bd of all functions deﬁned by (open) balls in Rd. Use the Dudley

4. Let P d

representation to ﬁgure out the VC-dimension of this class.
degree ≤ d, namely,

n denote the class of functions deﬁned by polynomial inequalities of

P d
n

= {h p : p is a polynomial of degree ≤ d in the variables x1, . . . , xn},
where for x= (x1. . . . , xn), h p(x)= 1[ p(x)≥0] (the degree of a multivariable poly-
nomial is the maximal sum of variable exponents over all of its terms. For
example, the degree of p(x) = 3x 3
1 x 2
2

+ 4x3x 2

7 is 5).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007

6.8 Exercises

57

1. Use the Dudley representation to ﬁgure out the VC-dimension of the class

P d
1 – the class of all d-degree polynomials over R.

2. Prove that the class of all polynomial classiﬁers over R has inﬁnite VC-

dimension.

3. Use the Dudley representation to ﬁgure out the VC-dimension of the class

P d
n (as a function of d and n).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:41:25, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.007


