21

Online Learning

In this chapter we describe a different model of learning, which is called online
learning. Previously, we studied the PAC learning model, in which the learner ﬁrst
receives a batch of training examples, uses the training set to learn a hypothesis,
and only when learning is completed uses the learned hypothesis for predicting
the label of new examples. In our papayas learning problem, this means that we
should ﬁrst buy a bunch of papayas and taste them all. Then, we use all of this
information to learn a prediction rule that determines the taste of new papayas. In
contrast, in online learning there is no separation between a training phase and a
prediction phase. Instead, each time we buy a papaya, it is ﬁrst considered a test
example since we should predict whether it is going to taste good. Then, after taking
a bite from the papaya, we know the true label, and the same papaya can be used
as a training example that can help us improve our prediction mechanism for future
papayas.

Concretely, online learning takes place in a sequence of consecutive rounds.
On each online round, the learner ﬁrst receives an instance (the learner buys a
papaya and knows its shape and color, which form the instance). Then, the learner
is required to predict a label (is the papaya tasty?). At the end of the round, the
learner obtains the correct label (he tastes the papaya and then knows whether
it is tasty or not). Finally, the learner uses this information to improve his future
predictions.

To analyze online learning, we follow a similar route to our study of PAC
learning. We start with online binary classiﬁcation problems. We consider both the
realizable case, in which we assume, as prior knowledge, that all the labels are gen-
erated by some hypothesis from a given hypothesis class, and the unrealizable case,
which corresponds to the agnostic PAC learning model. In particular, we present
an important algorithm called Weighted-Majority. Next, we study online learning
problems in which the loss function is convex. Finally, we present the Perceptron
algorithm as an example of the use of surrogate convex loss functions in the online
learning model.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

245

246

Online Learning

21.1 ONLINE CLASSIFICATION IN THE REALIZABLE CASE
Online learning is performed in a sequence of consecutive rounds, where at round
t the learner is given an instance, xt, taken from an instance domain X , and is
required to provide its label. We denote the predicted label by pt. After predicting
the label, the correct label, yt ∈ {0,1}, is revealed to the learner. The learner’s goal
is to make as few prediction mistakes as possible during this process. The learner
tries to deduce information from previous rounds so as to improve its predictions
on future rounds.

Clearly, learning is hopeless if there is no correlation between past and present
rounds. Previously in the book, we studied the PAC model in which we assume that
past and present examples are sampled i.i.d. from the same distribution source. In
the online learning model we make no statistical assumptions regarding the origin
of the sequence of examples. The sequence is allowed to be deterministic, stochas-
tic, or even adversarially adaptive to the learner’s own behavior (as in the case of
spam e-mail ﬁltering). Naturally, an adversary can make the number of predic-
tion mistakes of our online learning algorithm arbitrarily large. For example, the
adversary can present the same instance on each online round, wait for the learner’s
prediction, and provide the opposite label as the correct label.

To make nontrivial statements we must further restrict the problem. The real-
izability assumption is one possible natural restriction. In the realizable case, we
assume that all the labels are generated by some hypothesis, h(cid:7) : X → Y. Further-
more, h(cid:7) is taken from a hypothesis class H, which is known to the learner. This is
analogous to the PAC learning model we studied in Chapter 3. With this restriction
on the sequence, the learner should make as few mistakes as possible, assuming
that both h(cid:7) and the sequence of instances can be chosen by an adversary. For an
online learning algorithm, A, we denote by MA(H) the maximal number of mistakes
A might make on a sequence of examples which is labeled by some h(cid:7) ∈ H. We
emphasize again that both h(cid:7) and the sequence of instances can be chosen by an
adversary. A bound on MA(H) is called a mistake-bound and we will study how to
design algorithms for which MA(H) is minimal. Formally:
Deﬁnition 21.1 (Mistake Bounds, Online Learnability). Let H be a hypoth-
esis class and let A be an online learning algorithm. Given any sequence
S = (x1,h(cid:7)(y1)), . . . ,(xT ,h(cid:7)(yT )), where T is any integer and h(cid:7) ∈ H, let MA(S) be
the number of mistakes A makes on the sequence S. We denote by MA(H) the
supremum of MA(S) over all sequences of the preceding form. A bound of the form
MA(H)≤ B <∞ is called a mistake bound. We say that a hypothesis class H is online
learnable if there exists an algorithm A for which MA(H) ≤ B < ∞.

Our goal is to study which hypothesis classes are learnable in the online model,

and in particular to ﬁnd good learning algorithms for a given hypothesis class.
Remark 21.1. Throughout this section and the next, we ignore the computational
aspect of learning, and do not restrict the algorithms to be efﬁcient. In Section 21.3
and Section 21.4 we study efﬁcient online learning algorithms.

To simplify the presentation, we start with the case of a ﬁnite hypothesis class,

namely, |H| < ∞.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.1 Online Classiﬁcation in the Realizable Case

247

In PAC learning, we identiﬁed ERM as a good learning algorithm, in the sense
that if H is learnable then it is learnable by the rule ERMH. A natural learning rule
for online learning is to use (at any online round) any ERM hypothesis, namely, any
hypothesis which is consistent with all past examples.

Consistent

input: A ﬁnite hypothesis class H
initialize: V1 = H
for t = 1,2, . . .
receive xt
choose any h ∈ Vt
predict pt = h(xt)
receive true label yt = h(cid:7)(xt )
update Vt+1 = {h ∈ Vt : h(xt ) = yt}

The Consistent algorithm maintains a set, Vt, of all the hypotheses which are
consistent with (x1, y1), . . . ,(xt−1, yt−1). This set is often called the version space. It
then picks any hypothesis from Vt and predicts according to this hypothesis.
Obviously, whenever Consistent makes a prediction mistake, at least one hypoth-
esis is removed from Vt. Therefore, after making M mistakes we have |Vt|≤|H|− M.
Since Vt is always nonempty (by the realizability assumption it contains h(cid:7)) we have
1 ≤ |Vt| ≤ |H|− M. Rearranging, we obtain the following:
Corollary 21.2. Let H be a ﬁnite hypothesis class. The Consistent algorithm enjoys the
mistake bound MConsistent(H) ≤ |H|− 1.

It is rather easy to construct a hypothesis class and a sequence of examples on
which Consistent will indeed make |H|− 1 mistakes (see Exercise 21.1.) Therefore,
we present a better algorithm in which we choose h ∈ Vt in a smarter way. We shall
see that this algorithm is guaranteed to make exponentially fewer mistakes.

Halving
input: A ﬁnite hypothesis class H
initialize: V1 = H
for t = 1,2, . . .
receive xt
predict pt = argmaxr∈{0,1}|{h ∈ Vt : h(xt) = r}|
(in case of a tie predict pt = 1)
receive true label yt = h(cid:7)(xt )
update Vt+1 = {h ∈ Vt : h(xt ) = yt}

Theorem 21.3. Let H be a ﬁnite hypothesis class. The Halving algorithm enjoys the
mistake bound MHalving(H) ≤ log2 (|H|).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

248

Online Learning
Proof. We simply note that whenever the algorithm errs we have |Vt+1| ≤ |Vt|/2,
(hence the name Halving). Therefore, if M is the total number of mistakes, we have

−M.
Rearranging this inequality we conclude our proof.

1 ≤ |VT+1| ≤ |H|2

Of course, Halving’s mistake bound is much better than Consistent’s mistake
bound. We already see that online learning is different from PAC learning—while
in PAC, any ERM hypothesis is good, in online learning choosing an arbitrary ERM
hypothesis is far from being optimal.

21.1.1 Online Learnability
We next take a more general approach, and aim at characterizing online learnability.
In particular, we target the following question: What is the optimal online learning
algorithm for a given hypothesis class H?

We present a dimension of hypothesis classes that characterizes the best achiev-
able mistake bound. This measure was proposed by Nick Littlestone and we
therefore refer to it as Ldim(H).

To motivate the deﬁnition of Ldim it is convenient to view the online learning
process as a game between two players: the learner versus the environment. On
round t of the game, the environment picks an instance xt, the learner predicts a
label pt ∈ {0,1}, and ﬁnally the environment outputs the true label, yt ∈ {0,1}. Sup-
pose that the environment wants to make the learner err on the ﬁrst T rounds of the
game. Then, it must output yt = 1− pt, and the only question is how it should choose
the instances xt in such a way that ensures that for some h(cid:7) ∈ H we have yt = h(cid:7)(xt )
for all t ∈ [T ].
A strategy for an adversarial environment can be formally described as a binary
tree, as follows. Each node of the tree is associated with an instance from X . Initially,
the environment presents to the learner the instance associated with the root of the
tree. Then, if the learner predicts pt = 1 the environment will declare that this is a
wrong prediction (i.e., yt = 0) and will traverse to the right child of the current node.
If the learner predicts pt = 0 then the environment will set yt = 1 and will traverse
to the left child. This process will continue and at each round, the environment will
present the instance associated with the current node.
Formally, consider a complete binary tree of depth T (we deﬁne the depth of
the tree as the number of edges in a path from the root to a leaf). We have 2T+1 − 1
nodes in such a tree, and we attach an instance to each node. Let v1, . . . ,v2T+1−1 be
these instances. We start from the root of the tree, and set x1 = v1. At round t, we
set xt = vit where it is the current node. At the end of round t, we go to the left child
of it if yt = 0 or to the right child if yt = 1. That is, it+1 = 2it + yt. Unraveling the

recursion we obtain it = 2t−1 +(cid:2)

t−1
j=1 y j 2t−1− j.

The preceding strategy for the environment succeeds only if

for every
(y1, . . . , yT ) there exists h ∈ H such that yt = h(xt ) for all t ∈ [T ]. This leads to the
following deﬁnition.
Deﬁnition 21.4 (H Shattered Tree). A shattered tree of depth d is a sequence
of instances v1, . . . ,v2d−1 in X such that for every labeling (y1, . . . , yd) ∈ {0,1}d

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.1 Online Classiﬁcation in the Realizable Case

249

v1

v2

v3

h1
0
0
∗

h2
0
1
∗

h3
1
∗
0

h4
1
∗
1

v1
v2
v3

Figure 21.1. An illustration of a shattered tree of depth 2. The dashed path corresponds
to the sequence of examples ((v1,1),(v3,0)). The tree is shattered by H = {h1, h2, h3, h4},
where the predictions of each hypothesis in H on the instances v1,v2,v3 is given in the
table (the * mark means that h j (vi ) can be either 1 or 0).
(cid:2)
there exists h ∈ H such that for all t ∈ [d] we have h(vit ) = yt where it = 2t−1 +
t−1
j=1 y j 2t−1− j.
An illustration of a shattered tree of depth 2 is given in Figure 21.1.

Deﬁnition 21.5 (Littlestone’s Dimension (Ldim)). Ldim(H) is the maximal integer
T such that there exists a shattered tree of depth T , which is shattered by H.

The deﬁnition of Ldim and the previous discussion immediately imply the

following:

Lemma 21.6. No algorithm can have a mistake bound strictly smaller than
Ldim(H); namely, for every algorithm, A, we have MA(H) ≥ Ldim(H).
Proof. Let T = Ldim(H) and let v1, . . . ,v2T −1 be a sequence that satisﬁes the
requirements in the deﬁnition of Ldim. If the environment sets xt = vit and yt =
1− pt for all t ∈ [T ], then the learner makes T mistakes while the deﬁnition of Ldim
implies that there exists a hypothesis h ∈ H such that yt = h(xt ) for all t.

Let us now give several examples.

Example 21.2. Let H be a ﬁnite hypothesis class. Clearly, any tree that is shattered
by H has depth of at most log2 (|H|). Therefore, Ldim(H) ≤ log2 (|H|). Another way
to conclude this inequality is by combining Lemma 21.6 with Theorem 21.3.
Example 21.3. Let X = {1, . . . ,d} and H = {h1, . . . ,hd} where h j (x) = 1 iff x = j.
Then, it is easy to show that Ldim(H) = 1 while |H| = d can be arbitrarily large.
Therefore, this example shows that Ldim(H) can be signiﬁcantly smaller than
log2 (|H|).
Example 21.4. Let X = [0,1] and H = {x (cid:29)→ 1[x <a] : a ∈ [0,1]}; namely, H is the
class of thresholds on the interval [0,1]. Then, Ldim(H) = ∞. To see this, consider
the tree

1/2

1/4

3/4

1/8

3/8

5/8

7/8

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

250

Online Learning

This tree is shattered by H. And, because of the density of the reals, this tree can be
made arbitrarily deep.

Lemma 21.6 states that Ldim(H) lower bounds the mistake bound of any algo-
rithm. Interestingly, there is a standard algorithm whose mistake bound matches this
lower bound. The algorithm is similar to the Halving algorithm. Recall that the pre-
diction of Halving is made according to a majority vote of the hypotheses which are
consistent with previous examples. We denoted this set by Vt. Put another way, Halv-
t = {h ∈ Vt : h(xt ) = 0}.
−
ing partitions Vt into two sets: V
It then predicts according to the larger of the two groups. The rationale behind this
prediction is that whenever Halving makes a mistake it ends up with |Vt+1|≤ 0.5|Vt|.
The optimal algorithm we present in the following uses the same idea, but
instead of predicting according to the larger class, it predicts according to the class
with larger Ldim.

t = {h ∈ Vt : h(xt) = 1} and V
+

Standard Optimal Algorithm (SOA)

input: A hypothesis class H
initialize: V1 = H
for t = 1,2, . . .
receive xt
for r ∈ {0,1} let V (r)
predict pt = argmaxr∈{0,1} Ldim(V (r)
(in case of a tie predict pt = 1)
receive true label yt
update Vt+1 = {h ∈ Vt : h(xt ) = yt}

t

t = {h ∈ Vt : h(xt) = r}

)

The following lemma formally establishes the optimality of the preceding

algorithm.

Lemma 21.7.

SOA enjoys the mistake bound MSOA(H) ≤ Ldim(H).

Proof. It sufﬁces to prove that whenever the algorithm makes a prediction mistake
we have Ldim(Vt+1) ≤ Ldim(Vt )− 1. We prove this claim by assuming the contrary,
that is, Ldim(Vt+1) = Ldim(Vt). If this holds true, then the deﬁnition of pt implies
) = Ldim(Vt ) for both r = 1 and r = 0. But, then we can construct
that Ldim(V (r)
a shaterred tree of depth Ldim(Vt ) + 1 for the class Vt, which leads to the desired
contradiction.

t

Combining Lemma 21.7 and Lemma 21.6 we obtain:

Corollary 21.8. Let H be any hypothesis class. Then, the standard optimal algo-
rithm enjoys the mistake bound MSOA(H) = Ldim(H) and no other algorithm can
have MA(H) < Ldim(H).

Comparison to VC Dimension
In the PAC learning model, learnability is characterized by the VC dimension of
the class H. Recall that the VC dimension of a class H is the maximal number d
such that there are instances x1, . . . ,xd that are shattered by H. That is, for any

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

251

21.2 Online Classiﬁcation in the Unrealizable Case
sequence of labels (y1, . . . , yd) ∈ {0,1}d there exists a hypothesis h ∈ H that gives
exactly this sequence of labels. The following theorem relates the VC dimension to
the Littlestone dimension.
Theorem 21.9. For any class H, VCdim(H) ≤ Ldim(H), and there are classes for
which strict inequality holds. Furthermore, the gap can be arbitrarily larger.
Proof. We ﬁrst prove that VCdim(H) ≤ Ldim(H). Suppose VCdim(H) = d and
let x1, . . . ,xd be a shattered set. We now construct a complete binary tree of
instances v1, . . . ,v2d−1, where all nodes at depth i are set to be xi – see the following
illustration:

x1

x2

x2

x3

x3

x3

x3

Now, the deﬁnition of a shattered set clearly implies that we got a valid shattered
tree of depth d, and we conclude that VCdim(H) ≤ Ldim(H). To show that the gap
can be arbitrarily large simply note that the class given in Example 21.4 has VC
dimension of 1 whereas its Littlestone dimension is inﬁnite.

21.2 ONLINE CLASSIFICATION IN THE UNREALIZABLE CASE
In the previous section we studied online learnability in the realizable case. We now
consider the unrealizable case. Similarly to the agnostic PAC model, we no longer
assume that all labels are generated by some h(cid:7) ∈ H, but we require the learner to
be competitive with the best ﬁxed predictor from H. This is captured by the regret
of the algorithm, which measures how “sorry” the learner is, in retrospect, not to
have followed the predictions of some hypothesis h ∈ H. Formally, the regret of an
algorithm A relative to h when running on a sequence of T examples is deﬁned as

(cid:13)

T(cid:7)

t=1

| pt − yt|− T(cid:7)

t=1

(cid:15)

|h(xt)− yt|

,

(21.1)

RegretA(h, T ) =

sup

(x1,y1),...,(xT ,yT )

and the regret of the algorithm relative to a hypothesis class H is

RegretA(H, T ) = sup
h∈H

RegretA(h, T ).

(21.2)
We restate the learner’s goal as having the lowest possible regret relative to H. An
interesting question is whether we can derive an algorithm with low regret, meaning
that RegretA(H, T ) grows sublinearly with the number of rounds, T , which implies
that the difference between the error rate of the learner and the best hypothesis in
H tends to zero as T goes to inﬁnity.
We ﬁrst show that this is an impossible mission—no algorithm can obtain a
sublinear regret bound even if |H| = 2. Indeed, consider H = {h0,h1}, where h0
is the function that always returns 0 and h1 is the function that always returns 1. An

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

252

Online Learning

adversary can make the number of mistakes of any online algorithm be equal to T ,
by simply waiting for the learner’s prediction and then providing the opposite label
as the true label. In contrast, for any sequence of true labels, y1, . . . , yT , let b be
the majority of labels in y1, . . . , yT , then the number of mistakes of hb is at most T /2.
Therefore, the regret of any online algorithm might be at least T − T /2= T /2, which
is not sublinear in T . This impossibility result is attributed to Cover (Cover 1965).

To sidestep Cover’s impossibility result, we must further restrict the power of the
adversarial environment. We do so by allowing the learner to randomize his predic-
tions. Of course, this by itself does not circumvent Cover’s impossibility result, since
in deriving this result we assumed nothing about the learner’s strategy. To make the
randomization meaningful, we force the adversarial environment to decide on yt
without knowing the random coins ﬂipped by the learner on round t. The adversary
can still know the learner’s forecasting strategy and even the random coin ﬂips of
previous rounds, but it does not know the actual value of the random coin ﬂips used
by the learner on round t. With this (mild) change of game, we analyze the expected
number of mistakes of the algorithm, where the expectation is with respect to the
learner’s own randomization. That is, if the learner outputs ˆyt where P[ˆyt = 1] = pt,
then the expected loss he pays on round t is

P[ˆyt (cid:18)= yt] = | pt − yt|.

Put another way, instead of having the predictions of the learner being in {0,1} we
allow them to be in [0,1], and interpret pt ∈ [0,1] as the probability to predict the
label 1 on round t.

With this assumption it is possible to derive a low regret algorithm. In particular,

T(cid:7)

we will prove the following theorem.
Theorem 21.10. For every hypothesis class H, there exists an algorithm for online
classiﬁcation, whose predictions come from [0,1], that enjoys the regret bound
∀h ∈ H,
(cid:16)(cid:22)
Ldim(H) T

Furthermore, no algorithm can achieve an expected regret bound smaller than
(cid:6)

(cid:22)
2 min{log(|H|) , Ldim(H)log(eT )} T .

| pt − yt|− T(cid:7)
(cid:17)

|h(xt )− yt| ≤

t=1

t=1

.

We will provide a constructive proof of the upper bound part of the preceding
theorem. The proof of the lower bound part can be found in (Ben-David, Pal, &
Shalev-Shwartz 2009).

The proof of Theorem 21.10 relies on the Weighted-Majority algorithm for learn-
ing with expert advice. This algorithm is important by itself and we dedicate the next
subsection to it.

21.2.1 Weighted-Majority
Weighted-majority is an algorithm for the problem of prediction with expert advice.
In this online learning problem, on round t the learner has to choose the advice
of d given experts. We also allow the learner to randomize his choice by deﬁn-
ing a distribution over the d experts, that is, picking a vector w(t) ∈ [0,1]d, with

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

i

i

w(t)

21.2 Online Classiﬁcation in the Unrealizable Case

= 1, and choosing the ith expert with probability w(t)

(cid:2)
. After the learner
chooses an expert, it receives a vector of costs, vt ∈ [0,1]d, where vt ,i is the cost of
following the advice of the ith expert. If the learner’s predictions are randomized,
vt ,i = (cid:7)w(t),vt(cid:8). The
then its loss is deﬁned to be the averaged cost, namely,
algorithm assumes that the number of rounds T is given. In Exercise 21.4 we show
how to get rid of this dependence using the doubling trick.

(cid:2)

w(t)

i

i

i

253

2 log(d)/T

Weighted-Majority

parameter: η =(cid:22)
set w(t) = ˜w(t)/Zt where Zt =(cid:2)

input: number of experts, d ; number of rounds, T
initialize: ˜w(1) = (1, . . . ,1)
for t = 1,2, . . .
˜w(t)
choose expert i at random according to P[i] = w(t)
receive costs of all experts vt ∈ [0,1]d
pay cost (cid:7)w(t),vt(cid:8)
update rule ∀i , ˜w(t+1)

−ηvt,i

= ˜w(t)
i e

i

i

i

i

The following theorem is key for analyzing the regret bound of Weighted-

Majority.

Theorem 21.11. Assuming that T > 2log(d), the Weighted-Majority algorithm enjoys
the bound

T(cid:7)
(cid:7)w(t),vt(cid:8)− min
i∈[d]

t=1

T(cid:7)

t=1

vt ,i ≤

2 log(d) T .

Proof. We have:

log

(cid:2)

Using the inequality e
that

= 1, we obtain

w(t)

i

i

(cid:7)

˜w(t)
i
Zt

= log

−ηvt,i = log

Zt+1
Zt
−a ≤ 1 − a + a2/2, which holds for all a ∈ (0,1), and the fact

−ηvt,i .

w(t)
i e

e

i

i

(cid:22)

(cid:7)

log

Zt+1
Zt

≤ log

(cid:7)

w(t)

i

(cid:16)
(cid:7)
:

i

i

= log(1−

1− ηvt ,i + η2v2
(cid:16)

t ,i

/2

ηvt ,i − η2v2
;<

t ,i

w(t)

i

/2

).

(cid:17)
(cid:17)
=

Next, note that b ∈ (0,1). Therefore, taking log of the two sides of the inequality
1 − b ≤ e
−b we obtain the inequality log(1 − b) ≤ −b, which holds for all b ≤ 1,

def= b

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

254

Online Learning

and obtain

log

≤ −

Zt+1
Zt

(cid:7)

w(t)

i

i

(cid:16)
ηvt ,i − η2v2
(cid:7)

t ,i

(cid:17)

/2

w(t)

i

v2
t ,i

/2

i

= −η(cid:7)w(t),vt(cid:8)+ η2
≤ −η(cid:7)w(t),vt(cid:8)+ η2/2.
T(cid:7)

≤ −η

Zt+1
Zt

t=1

t=1

Summing this inequality over t we get

log(Z T+1)− log(Z1) = T(cid:7)

log

Next, we lower bound Z T+1. For each i, we can rewrite ˜w(T+1)
(cid:21)
get that

(cid:27)

(cid:20)

i

(cid:26)(cid:7)

(cid:2)

−η

vt,i

t

≥ log

(cid:2)

−η

vt,i

t

max

i

e

log Z T+1 = log

(cid:7)w(t),vt(cid:8)+ T η2
2
= e

.

(21.3)

vt,i and we

−η

(cid:2)

t

(cid:7)

= −η min

i

t

vt ,i .

Combining the preceding with Equation (21.3) and using the fact that log(Z1) =
log(d) we get that

−η min

i

vt ,i − log(d) ≤ − η

(cid:7)w(t),vt(cid:8)+ T η2
2

,

T(cid:7)

t=1

e

i

(cid:7)

t

which can be rearranged as follows:

T(cid:7)

t=1

(cid:7)w(t),vt(cid:8)− min

i

(cid:7)

t

vt ,i ≤ log(d)

η

+ η T
2

.

Plugging the value of η into the equation concludes our proof.

Proof of Theorem 21.10

Equipped with the Weighted-Majority algorithm and Theorem 21.11, we are ready to
prove Theorem 21.10. We start with the simpler case, in which H is a ﬁnite class,
and let us write H = {h1, . . . ,hd}. In this case, we can refer to each hypothesis, hi ,
as an expert, whose advice is to predict hi (xt ), and whose cost is vt ,i = |hi (xt )− yt|.
i hi (xt) ∈ [0,1], and the
w(t)

The prediction of the algorithm will therefore be pt =(cid:2)
(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) d(cid:7)

i (hi (xt)− yt)
w(t)

i hi (xt )− yt
w(t)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) d(cid:7)

| pt − yt| =

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) =

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) .

loss is

i

i=1

i=1

if yt = 1, then for all i, hi (xt ) − yt ≤ 0. Therefore, the above equals to
|hi (xt)− yt|. If yt = 0 then for all i, hi (xt )− yt ≥ 0, and the above also equals

(cid:2)

Now,

w(t)

i

i

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

(cid:2)

w(t)

i

i

|hi (xt)− yt|. All in all, we have shown that

21.2 Online Classiﬁcation in the Unrealizable Case

255

| pt − yt| = d(cid:7)

i=1

(cid:2)

|hi(xt )− yt| = (cid:7)w(t),vt(cid:8).

w(t)

i

vt ,i is exactly the number of mistakes hypothesis hi

Furthermore, for each i,
makes. Applying Theorem 21.11 we obtain
Corollary 21.12. Let H be a ﬁnite hypothesis class. There exists an algorithm for
online classiﬁcation, whose predictions come from [0,1], that enjoys the regret bound

t

T(cid:7)

t=1

| pt − yt|− min
h∈H

|h(xt)− yt| ≤

T(cid:7)

t=1

(cid:22)
2 log(|H|) T .

Next, we consider the case of a general hypothesis class. Previously, we con-
structed an expert for each individual hypothesis. However, if H is inﬁnite this leads
to a vacuous bound. The main idea is to construct a set of experts in a more sophis-
ticated way. The challenge is how to deﬁne a set of experts that, on one hand, is
not excessively large and, on the other hand, contains experts that give accurate
predictions.
We construct the set of experts so that for each hypothesis h ∈ H and every
sequence of instances, x1,x2, . . . ,xT , there exists at least one expert in the set which
behaves exactly as h on these instances. For each L ≤ Ldim(H) and each sequence
1≤ i1 < i2 <··· < i L ≤ T we deﬁne an expert. The expert simulates the game between
SOA (presented in the previous section) and the environment on the sequence
of instances x1,x2, . . . ,xT assuming that SOA makes a mistake precisely in rounds
i1,i2, . . . ,i L . The expert is deﬁned by the following algorithm.

Expert(i1,i2, . . . ,i L )

input A hypothesis class H ; Indices i1 < i2 < ··· < i L
initialize: V1 = H
for t = 1,2, . . . , T
(cid:16)
receive xt
for r ∈ {0,1} let V (r)
deﬁne ˜yt = argmaxr Ldim
(in case of a tie set ˜yt = 0)
t ∈ {i1,i2, . . . ,i L}

t = {h ∈ Vt : h(xt ) = r}

V (r)
t

(cid:17)

if

predict ˆyt = 1− ˜yt
else
predict ˆyt = ˜yt
update Vt+1 = V (ˆyt )

t

Note that each such expert can give us predictions at every round t while only
observing the instances x1, . . . ,xt . Our generic online learning algorithm is now an
application of the Weighted-Majority algorithm with these experts.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

256

Online Learning

d = Ldim(H)(cid:7)

L=0

(cid:20)

(cid:21)

T
L

To analyze the algorithm we ﬁrst note that the number of experts is

.

(21.4)

(cid:22)

It can be shown that when T ≥ Ldim(H) + 2, the right-hand side of the equation is
bounded by

(cid:4)Ldim(H) (the proof can be found in Lemma A.5).

(cid:3)
eT /Ldim(H)

Theorem 21.11 tells us that the expected number of mistakes of Weighted-
Majority is at most the number of mistakes of the best expert plus
2log(d) T . We
will next show that the number of mistakes of the best expert is at most the number
of mistakes of the best hypothesis in H. The following key lemma shows that, on
any sequence of instances, for each hypothesis h ∈ H there exists an expert with the
same behavior.
Lemma 21.13. Let H be any hypothesis class with Ldim(H) < ∞. Let x1,x2, . . . ,xT
be any sequence of instances. For any h ∈ H, there exists L ≤ Ldim(H) and indices
1≤ i1 < i2 <··· < i L ≤ T such that when running Expert(i1,i2, . . . ,i L) on the sequence
x1,x2, . . . ,xT , the expert predicts h(xt ) on each online round t = 1,2, . . . , T .
Proof. Fix h ∈ H and the sequence x1,x2, . . . ,xT . We must construct L and the
indices i1,i2, . . . ,i L . Consider running SOA on the input (x1,h(x1)), (x2,h(x2)), . . .,
(xT ,h(xT )). SOA makes at most Ldim(H) mistakes on such input. We deﬁne L to
be the number of mistakes made by SOA and we deﬁne {i1,i2, . . . ,i L} to be the set
of rounds in which SOA made the mistakes.

Now, consider the Expert(i1,i2, . . . ,i L ) running on the sequence x1,x2, . . . ,xT .
By construction, the set Vt maintained by Expert(i1,i2, . . . ,i L ) equals the set Vt
maintained by SOA when running on the sequence (x1,h(x1)), . . . ,(xT ,h(xT )). The
predictions of SOA differ from the predictions of h if and only if the round is
in {i1,i2, . . . ,i L}. Since Expert(i1,i2, . . . ,i L ) predicts exactly like SOA if t is not
in {i1,i2, . . . ,i L} and the opposite of SOAs’ predictions if t is in {i1,i2, . . . ,i L}, we
conclude that the predictions of the expert are always the same as the predic-
tions of h.

The previous lemma holds in particular for the hypothesis in H that makes the
least number of mistakes on the sequence of examples, and we therefore obtain the
following:

Corollary 21.14. Let (x1, y1),(x2, y2), . . . ,(xT , yT ) be a sequence of examples and let
H be a hypothesis class with Ldim(H) < ∞. There exists L ≤ Ldim(H) and indices
1 ≤ i1 < i2 < ··· < i L ≤ T , such that Expert(i1,i2, . . . ,i L ) makes at most as many
mistakes as the best h ∈ H does, namely,
T(cid:7)

min
h∈H

t=1

mistakes on the sequence of examples.

|h(xt )− yt|

Together with Theorem 21.11, the upper bound part of Theorem 21.10 is proven.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.3 Online Convex Optimization

257

21.3 ONLINE CONVEX OPTIMIZATION
In Chapter 12 we studied convex learning problems and showed learnability results
for these problems in the agnostic PAC learning framework. In this section we
show that similar learnability results hold for convex problems in the online learning
framework. In particular, we consider the following problem.

Online Convex Optimization

deﬁnitions:
hypothesis class H ; domain Z ; loss function (cid:9) : H× Z → R
assumptions:
H is convex
∀z ∈ Z, (cid:9)(·, z) is a convex function
for t = 1,2, . . . , T
learner predicts a vector w(t) ∈ H
environment responds with zt ∈ Z
learner suffers loss (cid:9)(w(t), zt )

As in the online classiﬁcation problem, we analyze the regret of the algorithm.
Recall that the regret of an online algorithm with respect to a competing hypothesis,
which here will be some vector w(cid:7) ∈ H, is deﬁned as

RegretA(w(cid:7), T ) = T(cid:7)

(cid:9)(w(t), zt )− T(cid:7)

t=1

t=1

(cid:9)(w(cid:7), zt ).

(21.5)

As before, the regret of the algorithm relative to a set of competing vectors, H, is
deﬁned as

RegretA(H, T ) = sup
w(cid:7)∈H

RegretA(w(cid:7), T ).

In Chapter 14 we have shown that Stochastic Gradient Descent solves convex
learning problems in the agnostic PAC model. We now show that a very similar
algorithm, Online Gradient Descent, solves online convex learning problems.

Online Gradient Descent

parameter: η > 0
initialize: w(1) = 0
for t = 1,2, . . . , T
predict w(t)
receive zt and let ft (· ) = (cid:9)(·, zt )
choose vt ∈ ∂ ft (w(t))
update:
2 ) = w(t) − ηvt
1. w(t+ 1
2 )(cid:9)
2. w(t+1) = argminw∈H(cid:9)w− w(t+ 1

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

258

Online Learning

Theorem 21.15. The Online Gradient Descent algorithm enjoys the following regret
bound for every w(cid:7) ∈ H,

RegretA(w(cid:7), T ) ≤ (cid:9)w(cid:7)(cid:9)2

2η

+ η
2

(cid:9)vt(cid:9)2.

T(cid:7)

t=1

√
If we further assume that ft is ρ-Lipschitz for all t, then setting η = 1/

T yields

T .
If we further assume that H is B-bounded and we set η = B
√

RegretA(w(cid:7), T ) ≤ 1
2

√
((cid:9)w(cid:7)(cid:9)2 + ρ2)

then

ρ

T

RegretA(H, T ) ≤ B ρ

√

T .

Proof. The analysis is similar to the analysis of Stochastic Gradient Descent with
projections. Using the projection lemma, the deﬁnition of w(t+ 1
2 ), and the deﬁnition
of subgradients, we have that for every t,

2 ) − w(cid:7)(cid:9)2 +(cid:9)w(t+ 1

2 ) − w(cid:7)(cid:9)2 −(cid:9)w(t) − w(cid:7)(cid:9)2

2 ) − w(cid:7)(cid:9)2 −(cid:9)w(t) − w(cid:7)(cid:9)2

(cid:9)w(t+1) − w(cid:7)(cid:9)2 −(cid:9)w(t) − w(cid:7)(cid:9)2
= (cid:9)w(t+1) − w(cid:7)(cid:9)2 −(cid:9)w(t+ 1
≤ (cid:9)w(t+ 1
= (cid:9)w(t) − ηvt − w(cid:7)(cid:9)2 −(cid:9)w(t) − w(cid:7)(cid:9)2
= −2η(cid:7)w(t) − w(cid:7),vt(cid:8)+ η2(cid:9)vt(cid:9)2
≤ −2η( ft (w(t))− ft (w(cid:7)))+ η2(cid:9)vt(cid:9)2.
T(cid:7)

Summing over t and observing that the left-hand side is a telescopic sum we
obtain that

( ft (w(t))− ft (w(cid:7)))+ η2
t=1
Rearranging the inequality and using the fact that w(1) = 0, we get that
T(cid:7)

(cid:9)w(T+1) − w(cid:7)(cid:9)2 −(cid:9)w(1) − w(cid:7)(cid:9)2 ≤ −2η
T(cid:7)

( ft (w(t))− ft (w(cid:7))) ≤ (cid:9)w(1) − w(cid:7)(cid:9)2 −(cid:9)w(T+1) − w(cid:7)(cid:9)2

t=1

+ η
2

t=1

(cid:9)vt(cid:9)2

t=1

T(cid:7)

(cid:9)vt(cid:9)2.

≤ (cid:9)w(cid:7)(cid:9)2

2η

+ η
2

2η

T(cid:7)

t=1

(cid:9)vt(cid:9)2.

This proves the ﬁrst bound in the theorem. The second bound follows from the
assumption that ft is ρ-Lipschitz, which implies that (cid:9)vt(cid:9) ≤ ρ.

21.4 THE ONLINE PERCEPTRON ALGORITHM
The Perceptron is a classic online learning algorithm for binary classiﬁcation with
the hypothesis class of homogenous halfspaces, namely, H = {x (cid:29)→ sign((cid:7)w,x(cid:8)) :

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.4 The Online Perceptron Algorithm 259

w ∈ Rd}. In Section 9.1.2 we have presented the batch version of the Perceptron,
which aims to solve the ERM problem with respect to H. We now present an online
version of the Perceptron algorithm.
Let X = Rd, Y = {−1,1}. On round t, the learner receives a vector xt ∈ Rd. The
learner maintains a weight vector w(t) ∈ Rd and predicts pt = sign((cid:7)w(t),xt(cid:8)). Then,
it receives yt ∈ Y and pays 1 if pt (cid:18)= yt and 0 otherwise.

The goal of the learner is to make as few prediction mistakes as possible. In
Section 21.1 we characterized the optimal algorithm and showed that the best
achievable mistake bound depends on the Littlestone dimension of the class. We
show later that if d ≥ 2 then Ldim(H) = ∞, which implies that we have no hope
of making few prediction mistakes. Indeed, consider the tree for which v1 =
,1,0, . . . ,0), v2 = ( 1
( 1
,1,0, . . . ,0), etc. Because of the density
of the reals, this tree is shattered by the subset of H which contains all hypothe-
2
ses that are parametrized by w of the form w = ( − 1,a,0, . . . ,0), for a ∈ [0,1]. We
conclude that indeed Ldim(H) = ∞.

,1,0, . . . ,0), v3 = ( 3

4

4

To sidestep this impossibility result, the Perceptron algorithm relies on the tech-
nique of surrogate convex losses (see Section 12.3). This is also closely related to the
notion of margin we studied in Chapter 15.
(cid:7)w,x(cid:8) does not equal y. Therefore, we can write the 0−1 loss function as follows

A weight vector w makes a mistake on an example (x, y) whenever the sign of

(cid:9)(w,(x, y)) = 1[y(cid:7)w,x(cid:8)≤0].

On rounds on which the algorithm makes a prediction mistake, we shall use the
hinge-loss as a surrogate convex loss function

ft (w) = max{0,1− yt(cid:7)w,xt(cid:8)}.

The hinge-loss satisﬁes the two conditions:
(cid:2) ft is a convex function
(cid:2) For all w, ft (w) ≥ (cid:9)(w,(xt , yt )). In particular, this holds for w(t).
On rounds on which the algorithm is correct, we shall deﬁne ft (w) = 0. Clearly, ft
is convex in this case as well. Furthermore, ft (w(t)) = (cid:9)(w(t),(xt , yt )) = 0.
Remark 21.5. In Section 12.3 we used the same surrogate loss function for all the
examples. In the online model, we allow the surrogate to depend on the speciﬁc
round. It can even depend on w(t). Our ability to use a round speciﬁc surrogate
stems from the worst-case type of analysis we employ in online learning.

Let us now run the Online Gradient Descent algorithm on the sequence of func-
f1, . . . , fT , with the hypothesis class being all vectors in Rd (hence, the
tions,
projection step is vacuous). Recall that the algorithm initializes w(1) = 0 and its
update rule is

w(t+1) = w(t) − ηvt

for some vt ∈ ∂ ft (w(t)). In our case, if yt(cid:7)w(t),xt(cid:8) > 0 then ft is the zero function and
we can take vt = 0. Otherwise, it is easy to verify that vt = −ytxt is in ∂ ft (w(t)). We

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

260

Online Learning

therefore obtain the update rule

(cid:5)

w(t+1) =

if yt(cid:7)w(t),xt(cid:8) > 0

w(t)
w(t) + ηytxt otherwise

Denote by M the set of rounds in which sign((cid:7)w(t),xt(cid:8)) (cid:18)= yt. Note that on round t,
the prediction of the Perceptron can be rewritten as

pt = sign((cid:7)w(t),xt(cid:8)) = sign

η

(cid:26)

(cid:7)

(cid:27)
yi (cid:7)xi ,xt(cid:8)

.

i∈M:i <t

This form implies that the predictions of the Perceptron algorithm and the set M
do not depend on the actual value of η as long as η > 0. We have therefore obtained
the Perceptron algorithm:

Perceptron

initialize: w1 = 0
for t = 1,2, . . . , T
receive xt
predict pt = sign((cid:7)w(t),xt(cid:8))
if yt(cid:7)w(t),xt(cid:8) ≤ 0
w(t+1) = w(t) + ytxt
else
w(t+1) = w(t)

To analyze the Perceptron, we rely on the analysis of Online Gradient Descent
ft we use in the
given in the previous section. In our case, the subgradient of
[yt(cid:7)w(t),xt(cid:8)≤0] yt xt. Indeed, the Perceptron’s update is w(t+1) =
Perceptron is vt = −1
w(t) − vt, and as discussed before this is equivalent to w(t+1) = w(t) − ηvt for every
ft (w(t))− T(cid:7)
T(cid:7)
η > 0. Therefore, Theorem 21.15 tells us that
ft (w(cid:7)) ≤ 1
2η

(cid:9)w(cid:7)(cid:9)2

T(cid:7)

+ η
2

2

t=1

t=1
Since ft (w(t)) is a surrogate for the 0−1 loss we know that
Denote R = maxt (cid:9)xt(cid:9); then we obtain

t=1

(cid:9)vt(cid:9)2
2.
(cid:2)
t=1 ft (w(t)) ≥ |M|.

T

|M|− T(cid:7)

t=1

ft (w(cid:7)) ≤ 1
2η

(cid:9)w(cid:7)(cid:9)2

2

+ η
2

|M| R2

Setting η = (cid:9)w(cid:7)(cid:9)

R

√|M| and rearranging, we obtain

|M|− T(cid:7)
(cid:22)
|M|− R(cid:9)w(cid:7)(cid:9)

t=1

ft (w(cid:7)) ≤ 0.

(21.6)

This inequality implies

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.6 Bibliographic Remarks

261

Theorem 21.16. Suppose that
the Perceptron algorithm runs on a sequence
(x1, y1), . . . ,(xT , yT ) and let R = maxt (cid:9)xt(cid:9). Let M be the rounds on which the
Perceptron errs and let ft (w) = 1[t∈M] [1− yt(cid:7)w,xt(cid:8)]+. Then, for every w(cid:7)

(cid:7)

t

+(cid:7)

t

|M| ≤

ft (w(cid:7))+ R(cid:9)w(cid:7)(cid:9)

ft (w(cid:7))+ R2(cid:9)w(cid:7)(cid:9)2 .

In particular, if there exists w(cid:7) such that yt(cid:7)w(cid:7),xt(cid:8) ≥ 1 for all t then

|M| ≤ R2(cid:9)w(cid:7)(cid:9)2.
√

√
Proof. The theorem follows from Equation (21.6) and the following claim: Given
x ,b,c ∈ R+, the inequality x − b
c. The last
claim can be easily derived by analyzing the roots of the convex parabola Q(y) =
y2 − by − c.

x − c ≤ 0 implies that x ≤ c + b2 + b

The last assumption of Theorem 21.16 is called separability with large margin
(see Chapter 15). That is, there exists w(cid:7) that not only satisﬁes that the point xt lies
on the correct side of the halfspace, it also guarantees that xt is not too close to the
decision boundary. More speciﬁcally, the distance from xt to the decision boundary
is at least γ = 1/(cid:9)w(cid:7)(cid:9) and the bound becomes (R/γ )2.
When the separability assumption does not hold, the bound involves the term
[1− yt(cid:7)w(cid:7),xt(cid:8)]+ which measures how much the separability with margin require-
ment is violated.

As a last remark we note that there can be cases in which there exists some w(cid:7)
that makes zero errors on the sequence but the Perceptron will make many errors.
Indeed, this is a direct consequence of the fact that Ldim(H) = ∞. The way we
sidestep this impossibility result is by assuming more on the sequence of examples –
the bound in Theorem 21.16 will be meaningful only if the cumulative surrogate
loss,

t ft (w(cid:7)) is not excessively large.

(cid:2)

21.5 SUMMARY
In this chapter we have studied the online learning model. Many of the results we
derived for the PAC learning model have an analog in the online model. First, we
have shown that a combinatorial dimension, the Littlestone dimension, character-
izes online learnability. To show this, we introduced the SOA algorithm (for the
realizable case) and the Weighted-Majority algorithm (for the unrealizable case).
We have also studied online convex optimization and have shown that online gradi-
ent descent is a successful online learner whenever the loss function is convex and
Lipschitz. Finally, we presented the online Perceptron algorithm as a combination
of online gradient descent and the concept of surrogate convex loss functions.

21.6 BIBLIOGRAPHIC REMARKS
The Standard Optimal Algorithm was derived by the seminal work of Littlestone
(1988). A generalization to the nonrealizable case, as well as other variants like
margin-based Littlestone’s dimension, were derived in (Ben-David et al. 2009).
Characterizations of online learnability beyond classiﬁcation have been obtained

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

262

Online Learning

in (Abernethy, Bartlett, Rakhlin & Tewari 2008, Rakhlin, Sridharan & Tewari
2010, Daniely et al. 2011). The Weighted-Majority algorithm is due to (Littlestone
& Warmuth 1994) and (Vovk 1990).

The term “online convex programming” was introduced by Zinkevich (2003)
but this setting was introduced some years earlier by Gordon (1999). The Percep-
tron dates back to Rosenblatt (Rosenblatt 1958). An analysis for the realizable
case (with margin assumptions) appears in (Agmon 1954, Minsky & Papert 1969).
Freund and Schapire (Freund & Schapire 1999) presented an analysis for the
unrealizable case with a squared-hinge-loss based on a reduction to the realizable
case. A direct analysis for the unrealizable case with the hinge-loss was given by
Gentile (Gentile 2003).

For additional information we refer the reader to Cesa-Bianchi and Lugosi

(2006) and Shalev-Shwartz (2011).

|H|− 1 mistakes.

21.7 EXERCISES
21.1 Find a hypothesis class H and a sequence of examples on which Consistent makes
21.2 Find a hypothesis class H and a sequence of examples on which the mistake bound
of the Halving algorithm is tight.
21.3 Let d ≥ 2, X = {1, . . . , d} and let H = {h j : j ∈ [d]}, where h j (x) = 1[x= j]. Calculate
MHalving(H) (i.e., derive lower and upper bounds on MHalving(H), and prove that
they are equal).

21.4 The Doubling Trick:

In Theorem 21.15, the parameter η depends on the time horizon T . In this exercise
we show how to get rid of this dependence by a simple trick.

Consider an algorithm that enjoys a regret bound of the form α

T , but its
parameters require the knowledge of T . The doubling trick, described in the follow-
ing, enables us to convert such an algorithm into an algorithm that does not need
to know the time horizon. The idea is to divide the time into periods of increasing
size and run the original algorithm on each period.

√

The Doubling Trick

input: algorithm A whose parameters depend on the time horizon
for m = 0,1,2, . . .
run A on the 2m rounds t = 2m , . . . ,2m+1 − 1

√

2m, then the

Show that if the regret of A on each period of 2m rounds is at most α
total regret is at most

√
2√
2− 1

√

α

T .

21.5 Online-to-batch Conversions: In this exercise we demonstrate how a successful

online learning algorithm can be used to derive a successful PAC learner as well.
Consider a PAC learning problem for binary classiﬁcation parameterized by an
instance domain, X , and a hypothesis class, H. Suppose that there exists an online
learning algorithm, A, which enjoys a mistake bound MA(H) < ∞. Consider run-
ning this algorithm on a sequence of T examples which are sampled i.i.d. from a
distribution D over the instance space X , and are labeled by some h(cid:7) ∈ H. Suppose

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022

21.7 Exercises

263

that for every round t, the prediction of the algorithm is based on a hypothesis
ht : X → {0,1}. Show that

E[LD(hr )] ≤ MA(H)

,

T

where the expectation is over the random choice of the instances as well as a ran-
dom choice of r according to the uniform distribution over [T ].
Hint: Use similar arguments to the ones appearing in the proof of Theorem 14.8.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:57:32, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.022


