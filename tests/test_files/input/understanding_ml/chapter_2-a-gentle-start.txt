2

A Gentle Start

Let us begin our mathematical analysis by showing how successful learning can be
achieved in a relatively simpliﬁed setting. Imagine you have just arrived in some
small Paciﬁc island. You soon ﬁnd out that papayas are a signiﬁcant ingredient in the
local diet. However, you have never before tasted papayas. You have to learn how
to predict whether a papaya you see in the market is tasty or not. First, you need
to decide which features of a papaya your prediction should be based on. On the
basis of your previous experience with other fruits, you decide to use two features:
the papaya’s color, ranging from dark green, through orange and red to dark brown,
and the papaya’s softness, ranging from rock hard to mushy. Your input for ﬁguring
out your prediction rule is a sample of papayas that you have examined for color
and softness and then tasted and found out whether they were tasty or not. Let
us analyze this task as a demonstration of the considerations involved in learning
problems.

Our ﬁrst step is to describe a formal model aimed to capture such learning tasks.

2.1 A FORMAL MODEL – THE STATISTICAL LEARNING FRAMEWORK

to the following:

The learner’s input: In the basic statistical learning setting, the learner has access
Domain set: An arbitrary set, X . This is the set of objects that we may wish
to label. For example, in the papaya learning problem mentioned before,
the domain set will be the set of all papayas. Usually, these domain
points will be represented by a vector of features (like the papaya’s color
and softness). We also refer to domain points as instances and to X as
instance space.
Label set: For our current discussion, we will restrict the label set to be a
two-element set, usually {0,1} or {−1,+1}. Let Y denote our set of pos-
sible labels. For our papayas example, let Y be {0,1}, where 1 represents
being tasty and 0 stands for being not-tasty.
Training data: S = ((x1, y1) . . . (xm , ym)) is a ﬁnite sequence of pairs in X ×Y:
that is, a sequence of labeled domain points. This is the input that the

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

13

14

A Gentle Start

learner has access to (like a set of papayas that have been tasted and their
color, softness, and tastiness). Such labeled examples are often called
training examples. We sometimes also refer to S as a training set.1

The learner’s output: The learner is requested to output a prediction rule,
h : X → Y. This function is also called a predictor, a hypothesis, or a classiﬁer.
The predictor can be used to predict the label of new domain points. In our
papayas example, it is a rule that our learner will employ to predict whether
future papayas he examines in the farmers’ market are going to be tasty or not.
We use the notation A(S) to denote the hypothesis that a learning algorithm,
A, returns upon receiving the training sequence S.

A simple data-generation model We now explain how the training data is gen-
erated. First, we assume that the instances (the papayas we encounter) are
generated by some probability distribution (in this case, representing the
environment). Let us denote that probability distribution over X by D. It is
important to note that we do not assume that the learner knows anything about
this distribution. For the type of learning tasks we discuss, this could be any
arbitrary probability distribution. As to the labels, in the current discussion
we assume that there is some “correct” labeling function, f : X → Y, and that
yi = f (xi) for all i. This assumption will be relaxed in the next chapter. The
labeling function is unknown to the learner. In fact, this is just what the learner
is trying to ﬁgure out. In summary, each pair in the training data S is generated
by ﬁrst sampling a point xi according to D and then labeling it by f .

Measures of success: We deﬁne the error of a classiﬁer to be the probability that
it does not predict the correct label on a random data point generated by the
aforementioned underlying distribution. That is, the error of h is the proba-
bility to draw a random instance x, according to the distribution D, such that
h(x) does not equal f (x).
Formally, given a domain subset,2 A ⊂ X , the probability distribution, D,
assigns a number, D(A), which determines how likely it is to observe a point
x ∈ A. In many cases, we refer to A as an event and express it using a function
π : X → {0,1}, namely, A = {x ∈ X : π(x) = 1}. In that case, we also use the
notation Px∼D [π(x)] to express D(A).
We deﬁne the error of a prediction rule, h : X → Y, to be

LD, f (h) def= P

x∼D [h(x) (cid:18)= f (x)] def= D({x : h(x) (cid:18)= f (x)}).

(2.1)

That is, the error of such h is the probability of randomly choosing an example
x for which h(x) (cid:18)= f (x). The subscript (D, f ) indicates that the error is mea-
sured with respect to the probability distribution D and the correct labeling
function f . We omit this subscript when it is clear from the context. L(D, f )(h)
has several synonymous names such as the generalization error, the risk, or
the true error of h, and we will use these names interchangeably throughout

1 Despite the “set” notation, S is a sequence. In particular, the same example may appear twice in S and

some algorithms can take into account the order of examples in S.
2 Strictly speaking, we should be more careful and require that A is a member of some σ -algebra of
subsets of X , over which D is deﬁned. We will formally deﬁne our measurability assumptions in the
next chapter.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

2.2 Empirical Risk Minimization

15

the book. We use the letter L for the error, since we view this error as the loss
of the learner. We will later also discuss other possible formulations of such
loss.

A note about the information available to the learner The learner is blind to the
underlying distribution D over the world and to the labeling function f. In our
papayas example, we have just arrived in a new island and we have no clue
as to how papayas are distributed and how to predict their tastiness. The only
way the learner can interact with the environment is through observing the
training set.

In the next section we describe a simple learning paradigm for the preceding

setup and analyze its performance.

2.2 EMPIRICAL RISK MINIMIZATION
As mentioned earlier, a learning algorithm receives as input a training set S, sam-
pled from an unknown distribution D and labeled by some target function f , and
should output a predictor h S : X → Y (the subscript S emphasizes the fact that
the output predictor depends on S). The goal of the algorithm is to ﬁnd h S that
minimizes the error with respect to the unknown D and f .
Since the learner does not know what D and f are, the true error is not directly
available to the learner. A useful notion of error that can be calculated by the
learner is the training error – the error the classiﬁer incurs over the training sample:

L S(h) def= |{i ∈ [m] : h(xi ) (cid:18)= yi}|

m

,

(2.2)

where [m] = {1, . . . ,m}.

The terms empirical error and empirical risk are often used interchangeably for

this error.

Since the training sample is the snapshot of the world that is available to the
learner, it makes sense to search for a solution that works well on that data. This
learning paradigm – coming up with a predictor h that minimizes L S(h) – is called
Empirical Risk Minimization or ERM for short.

2.2.1 Something May Go Wrong – Overﬁtting
Although the ERM rule seems very natural, without being careful, this approach
may fail miserably.

To demonstrate such a failure, let us go back to the problem of learning to pre-
dict the taste of a papaya on the basis of its softness and color. Consider a sample as
depicted in the following:

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

16

A Gentle Start

Assume that the probability distribution D is such that instances are distributed
uniformly within the larger square and the labeling function, f , determines the label
to be 1 if the instance is within the inner square, and 0 otherwise. The area of the
larger square in the picture is 2 and the area of the inner square is 1. Consider the
following predictor:

(cid:5)

h S(x) =

if ∃i ∈ [m] s.t. xi = x
otherwise.

yi
0

(2.3)

While this predictor might seem rather artiﬁcial, in Exercise 2.1 we show a natu-
ral representation of it using polynomials. Clearly, no matter what the sample is,
L S(h S) = 0, and therefore this predictor may be chosen by an ERM algorithm (it is
one of the empirical-minimum-cost hypotheses; no classiﬁer can have smaller error).
On the other hand, the true error of any classiﬁer that predicts the label 1 only on a
ﬁnite number of instances is, in this case, 1/2. Thus, LD(h S) = 1/2. We have found
a predictor whose performance on the training set is excellent, yet its performance
on the true “world” is very poor. This phenomenon is called overﬁtting. Intuitively,
overﬁtting occurs when our hypothesis ﬁts the training data “too well” (perhaps like
the everyday experience that a person who provides a perfect detailed explanation
for each of his single actions may raise suspicion).

2.3 EMPIRICAL RISK MINIMIZATION WITH INDUCTIVE BIAS
We have just demonstrated that the ERM rule might lead to overﬁtting. Rather
than giving up on the ERM paradigm, we will look for ways to rectify it. We will
search for conditions under which there is a guarantee that ERM does not overﬁt,
namely, conditions under which when the ERM predictor has good performance
with respect to the training data, it is also highly likely to perform well over the
underlying data distribution.

A common solution is to apply the ERM learning rule over a restricted search
space. Formally, the learner should choose in advance (before seeing the data) a set
of predictors. This set is called a hypothesis class and is denoted by H. Each h ∈ H
is a function mapping from X to Y. For a given class H, and a training sample, S,
the ERMH learner uses the ERM rule to choose a predictor h ∈ H, with the lowest
possible error over S. Formally,

ERMH(S) ∈ argmin
h∈H

L S(h),

where argmin stands for the set of hypotheses in H that achieve the minimum value
of L S(h) over H. By restricting the learner to choosing a predictor from H, we bias it
toward a particular set of predictors. Such restrictions are often called an inductive
bias. Since the choice of such a restriction is determined before the learner sees the
training data, it should ideally be based on some prior knowledge about the problem
to be learned. For example, for the papaya taste prediction problem we may choose
the class H to be the set of predictors that are determined by axis aligned rectangles
(in the space determined by the color and softness coordinates). We will later show
that ERMH over this class is guaranteed not to overﬁt. On the other hand, the
example of overﬁtting that we have seen previously, demonstrates that choosing H

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

2.3 Empirical Risk Minimization with Inductive Bias

17

to be a class of predictors that includes all functions that assign the value 1 to a ﬁnite
set of domain points does not sufﬁce to guarantee that ERMH will not overﬁt.

A fundamental question in learning theory is, over which hypothesis classes
ERMH learning will not result in overﬁtting. We will study this question later in
the book.

Intuitively, choosing a more restricted hypothesis class better protects us against
overﬁtting but at the same time might cause us a stronger inductive bias. We will get
back to this fundamental tradeoff later.

2.3.1 Finite Hypothesis Classes
The simplest type of restriction on a class is imposing an upper bound on its size
(that is, the number of predictors h in H). In this section, we show that if H is a
ﬁnite class then ERMH will not overﬁt, provided it is based on a sufﬁciently large
training sample (this size requirement will depend on the size of H).
Limiting the learner to prediction rules within some ﬁnite hypothesis class may
be considered as a reasonably mild restriction. For example, H can be the set of all
predictors that can be implemented by a C++ program written in at most 109 bits
of code. In our papayas example, we mentioned previously the class of axis aligned
rectangles. While this is an inﬁnite class, if we discretize the representation of real
numbers, say, by using a 64 bits ﬂoating-point representation, the hypothesis class
becomes a ﬁnite class.
Let us now analyze the performance of the ERMH learning rule assuming that
H is a ﬁnite class. For a training sample, S, labeled according to some f : X → Y, let
h S denote a result of applying ERMH to S, namely,

h S ∈ argmin
h∈H

L S(h).

(2.4)

In this chapter, we make the following simplifying assumption (which will be

relaxed in the next chapter).
Deﬁnition 2.1 (The Realizability Assumption). There exists h(cid:12) ∈ H s.t.
L(D, f )(h(cid:12)) = 0. Note that this assumption implies that with probability 1 over ran-
dom samples, S, where the instances of S are sampled according to D and are labeled
by f , we have L S(h(cid:12)) = 0.

The realizability assumption implies that for every ERM hypothesis we have
that3 L S(h S) = 0. However, we are interested in the true risk of h S, L(D, f )(h S),
rather than its empirical risk.
Clearly, any guarantee on the error with respect to the underlying distribution,
D, for an algorithm that has access only to a sample S should depend on the rela-
tionship between D and S. The common assumption in statistical machine learning
is that the training sample S is generated by sampling points from the distribution D
independently of each other. Formally,

3 Mathematically speaking, this holds with probability 1. To simplify the presentation, we sometimes

omit the “with probability 1” speciﬁer.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

18

A Gentle Start

The i.i.d. assumption: The examples in the training set are independently and
identically distributed (i.i.d.) according to the distribution D. That is, every
xi in S is freshly sampled according to D and then labeled according to the
f . We denote this assumption by S ∼ Dm where m is the
labeling function,
size of S, and Dm denotes the probability over m-tuples induced by applying D
to pick each element of the tuple independently of the other members of the
tuple.
Intuitively, the training set S is a window through which the learner gets
partial information about the distribution D over the world and the labeling
function,
f . The larger the sample gets, the more likely it is to reﬂect more
accurately the distribution and labeling used to generate it.

Since L(D, f )(h S) depends on the training set, S, and that training set is picked by
a random process, there is randomness in the choice of the predictor h S and, conse-
quently, in the risk L(D, f )(h S). Formally, we say that it is a random variable. It is not
realistic to expect that with full certainty S will sufﬁce to direct the learner toward
a good classiﬁer (from the point of view of D), as there is always some probability
that the sampled training data happens to be very nonrepresentative of the under-
lying D. If we go back to the papaya tasting example, there is always some (small)
chance that all the papayas we have happened to taste were not tasty, in spite of the
fact that, say, 70% of the papayas in our island are tasty. In such a case, ERMH(S)
may be the constant function that labels every papaya as “not tasty” (and has 70%
error on the true distribution of papapyas in the island). We will therefore address
the probability to sample a training set for which L(D, f )(h S) is not too large. Usu-
ally, we denote the probability of getting a nonrepresentative sample by δ, and call
(1− δ) the conﬁdence parameter of our prediction.

On top of that, since we cannot guarantee perfect label prediction, we introduce
another parameter for the quality of prediction, the accuracy parameter, commonly
denoted by (cid:14). We interpret the event L(D, f )(h S) > (cid:14) as a failure of the learner, while
if L(D, f )(h S) ≤ (cid:14) we view the output of the algorithm as an approximately correct
predictor. Therefore (ﬁxing some labeling function f : X → Y), we are interested
in upper bounding the probability to sample m-tuple of instances that will lead to
failure of the learner. Formally, let S|x = (x1, . . . , xm) be the instances of the training
set. We would like to upper bound

Dm({S|x : L(D, f )(h S) > (cid:14)}).

Let HB be the set of “bad” hypotheses, that is,

HB = {h ∈ H : L(D, f )(h) > (cid:14)}.

In addition, let

M = {S|x : ∃h ∈ HB , L S(h) = 0}

be the set of misleading samples: Namely, for every S|x ∈ M, there is a “bad” hypoth-
esis, h ∈ HB, that looks like a “good” hypothesis on S|x. Now, recall that we would
like to bound the probability of the event L(D, f )(h S) > (cid:14). But, since the realizabil-
ity assumption implies that L S(h S) = 0, it follows that the event L(D, f )(h S) > (cid:14) can
only happen if for some h ∈ HB we have L S(h) = 0. In other words, this event will

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

2.3 Empirical Risk Minimization with Inductive Bias

19

only happen if our sample is in the set of misleading samples, M. Formally, we have
shown that

{S|x : L(D, f )(h S) > (cid:14)} ⊆ M.

{S|x : L S(h) = 0}.

(2.5)

Note that we can rewrite M as

M =

Hence,

(cid:6)

h∈HB

Dm({S|x : L(D, f )(h S) > (cid:14)}) ≤ Dm(M) = Dm(∪h∈HB
Next, we upper bound the right-hand side of the preceding equation using the

{S|x : L S(h) = 0}).

(2.6)

union bound – a basic property of probabilities.
Lemma 2.2 (Union Bound). For any two sets A, B and a distribution D we have

Applying the union bound to the right-hand side of Equation (2.6) yields

Dm({S|x : L(D, f )(h S) > (cid:14)}) ≤

Dm({S|x : L S(h) = 0}).

(2.7)

D(A ∪ B) ≤ D(A)+D(B).

(cid:7)

h∈HB

Next, let us bound each summand of the right-hand side of the preceding inequality.
Fix some “bad” hypothesis h ∈ HB. The event L S(h) = 0 is equivalent to the event
∀i ,h(xi ) = f (xi ). Since the examples in the training set are sampled i.i.d. we get that

Dm({S|x : L S(h) = 0}) = Dm({S|x : ∀i ,h(xi ) = f (xi )})
D({xi : h(xi ) = f (xi )}).

= m(cid:8)

i=1

(2.8)

For each individual sampling of an element of the training set we have

D({xi : h(xi ) = yi}) = 1− L(D, f )(h) ≤ 1− (cid:14),

where the last inequality follows from the fact that h ∈ HB. Combining the previous
equation with Equation (2.8) and using the inequality 1− (cid:14) ≤ e
−(cid:14) we obtain that for
every h ∈ HB,

Dm({S|x : L S(h) = 0}) ≤ (1− (cid:14))m ≤ e

−(cid:14)m.
Combining this equation with Equation (2.7) we conclude that
−(cid:14)m ≤ |H| e

Dm({S|x : L(D, f )(h S) > (cid:14)}) ≤ |HB| e

(2.9)

−(cid:14) m.

A graphical illustration which explains how we used the union bound is given in
Figure 2.1.
Corollary 2.3. Let H be a ﬁnite hypothesis class. Let δ ∈ (0,1) and (cid:14) > 0 and let m be
an integer that satisﬁes

m ≥ log(|H|/δ)

.

(cid:14)

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

20

A Gentle Start

Figure 2.1. Each point in the large circle represents a possible m-tuple of instances. Each
colored oval represents the set of “misleading” m-tuple of instances for some “bad” pre-
dictor h ∈ HB. The ERM can potentially overﬁt whenever it gets a misleading training set
S. That is, for some h ∈ HB we have L S(h) = 0. Equation (2.9) guarantees that for each
individual bad hypothesis, h ∈ HB, at most (1− (cid:14))m-fraction of the training sets would be
misleading. In particular, the larger m is, the smaller each of these colored ovals becomes.
The union bound formalizes the fact that the area representing the training sets that are
misleading with respect to some h ∈ HB (that is, the training sets in M) is at most the
sum of the areas of the colored ovals. Therefore, it is bounded by |HB| times the maximum
size of a colored oval. Any sample S outside the colored ovals cannot cause the ERM rule
to overﬁt.
Then, for any labeling function, f , and for any distribution, D, for which the realiz-
ability assumption holds (that is, for some h ∈ H, L(D, f )(h) = 0), with probability of
at least 1 − δ over the choice of an i.i.d. sample S of size m, we have that for every
ERM hypothesis, h S, it holds that

L(D, f )(h S) ≤ (cid:14).

The preceeding corollary tells us that for a sufﬁciently large m, the ERMH rule
over a ﬁnite hypothesis class will be probably (with conﬁdence 1− δ) approximately
(up to an error of (cid:14)) correct. In the next chapter we formally deﬁne the model of
Probably Approximately Correct (PAC) learning.

2.4 EXERCISES
2.1 Overﬁtting of polynomial matching: We have shown that the predictor deﬁned in
Equation (2.3) leads to overﬁtting. While this predictor seems to be very unnatural,
the goal of this exercise is to show that it can be described as a thresholded poly-
nomial. That is, show that given a training set S = {(xi , f (xi ))}m
⊆ (Rd × {0,1})m,
i=1
there exists a polynomial pS such that h S(x) = 1 if and only if pS(x) ≥ 0, where h S
is as deﬁned in Equation (2.3). It follows that learning the class of all thresholded
polynomials using the ERM rule may lead to overﬁtting.
2.2 Let H be a class of binary classiﬁers over a domain X . Let D be an unknown distri-
bution over X , and let f be the target hypothesis in H. Fix some h ∈ H. Show that
the expected value of L S(h) over the choice of S|x equals L(D, f )(h), namely,

[L S(h)] = L(D, f )(h).

E

S|x∼Dm

2.3 Axis aligned rectangles: An axis aligned rectangle classiﬁer in the plane is a classi-
ﬁer that assigns the value 1 to a point if and only if it is inside a certain rectangle.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003

2.4 Exercises

21

-

+

R*

+
R(S)

+

-

+

+

R1

Figure 2.2. Axis aligned rectangles.

Formally, given real numbers a1 ≤ b1, a2 ≤ b2, deﬁne the classiﬁer h(a1,b1 ,a2,b2) by

h(a1,b1,a2,b2)(x1, x2) =

1 if a1 ≤ x1 ≤ b1 and a2 ≤ x2 ≤ b2
0 otherwise

.

(2.10)

(cid:5)

The class of all axis aligned rectangles in the plane is deﬁned as
= {h(a1,b1 ,a2,b2) : a1 ≤ b1, and a2 ≤ b2}.

H2
rec

(cid:14)

, b

Note that this is an inﬁnite size hypothesis class. Throughout this exercise we rely
on the realizability assumption.
1. Let A be the algorithm that returns the smallest rectangle enclosing all positive
examples in the training set. Show that A is an ERM.
2. Show that if A receives a training set of size ≥ 4log (4/δ)
then, with probability of
at least 1− δ it returns a hypothesis with error of at most (cid:14).
∗ = R(a
∗
∗
∗
∗
Hint: Fix some distribution D over X , let R
2) be the rectan-
, a
, b
2
1
1
gle that generates the labels, and let f be the corresponding hypothesis. Let
a1 ≥ a
∗
1 be a number such that the probability mass (with respect to D) of the
rectangle R1 = R(a
∗
2) is exactly (cid:14)/4. Similarly, let b1, a2, b2 be numbers
2), R3 =
such that the probability masses of the rectangles R2 = R(b1, b
∗
∗
∗
∗
2) are all exactly (cid:14)/4. Let R(S) be the
R(a
, b
1
1
rectangle returned by A. See illustration in Figure 2.2.
(cid:2) Show that R(S) ⊆ R
(cid:2) Show that if S contains (positive) examples in all of the rectangles
(cid:2) For each i ∈ {1, . . . ,4}, upper bound the probability that S does not contain
(cid:2) Use the union bound to conclude the argument.

∗
, b
, a1, a
2
, a2), R4 = R(a
∗
1
∗

R1, R2, R3, R4, then the hypothesis returned by A has error of at most (cid:14).

an example from Ri .

∗
, a
2

∗
, a
2

∗
, b
1

, b2, b

3. Repeat the previous question for the class of axis aligned rectangles in Rd.
4. Show that the runtime of applying the algorithm A mentioned earlier is polyno-

, b

∗
1

∗
1

.

mial in d,1/(cid:14), and in log(1/δ).

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:36:47, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.003


