4

Learning via Uniform Convergence

The ﬁrst formal learning model that we have discussed was the PAC model. In
Chapter 2 we have shown that under the realizability assumption, any ﬁnite hypoth-
esis class is PAC learnable. In this chapter we will develop a general tool, uniform
convergence, and apply it to show that any ﬁnite class is learnable in the agnos-
tic PAC model with general loss functions, as long as the range loss function is
bounded.

4.1 UNIFORM CONVERGENCE IS SUFFICIENT FOR LEARNABILITY
The idea behind the learning condition discussed in this chapter is very simple.
Recall that, given a hypothesis class, H, the ERM learning paradigm works as fol-
lows: Upon receiving a training sample, S, the learner evaluates the risk (or error)
of each h in H on the given sample and outputs a member of H that minimizes this
empirical risk. The hope is that an h that minimizes the empirical risk with respect to
S is a risk minimizer (or has risk close to the minimum) with respect to the true data
probability distribution as well. For that, it sufﬁces to ensure that the empirical risks
of all members of H are good approximations of their true risk. Put another way, we
need that uniformly over all hypotheses in the hypothesis class, the empirical risk
will be close to the true risk, as formalized in the following.

Deﬁnition 4.1 ((cid:14)-representative sample). A training set S is called (cid:14)-representative
(w.r.t. domain Z, hypothesis class H, loss function (cid:2), and distribution D) if

∀h ∈ H, |L S(h)− LD(h)| ≤ (cid:14).

The next simple lemma states that whenever the sample is ((cid:14)/2)-representative,

the ERM learning rule is guaranteed to return a good hypothesis.

2 -representative (w.r.t. domain Z,
loss function (cid:2), and distribution D). Then, any output of

Lemma 4.2. Assume that a training set S is (cid:14)
hypothesis class H,
ERMH(S), namely, any h S ∈ argminh∈H L S(h), satisﬁes
h∈H LD(h)+ (cid:14).

LD(h S) ≤ min

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:38:38, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.005

31

32

Learning via Uniform Convergence
Proof. For every h ∈ H,

LD(h S) ≤ L S(h S)+ (cid:14)

≤ L S(h)+ (cid:14)

≤ LD(h)+ (cid:14)

2

+ (cid:14)

2

= LD(h)+ (cid:14),

2

2

where the ﬁrst and third inequalities are due to the assumption that S is
(cid:14)
2 -representative (Deﬁnition 4.1) and the second inequality holds since h S is an ERM
predictor.

The preceding lemma implies that to ensure that the ERM rule is an agnostic
PAC learner, it sufﬁces to show that with probability of at least 1− δ over the ran-
dom choice of a training set, it will be an (cid:14)-representative training set. The uniform
convergence condition formalizes this requirement.
Deﬁnition 4.3 (Uniform Convergence). We say that a hypothesis class H has the
uniform convergence property (w.r.t. a domain Z and a loss function (cid:2)) if there exists
a function mUCH : (0,1)2 → N such that for every (cid:14), δ ∈ (0,1) and for every probabil-
ity distribution D over Z, if S is a sample of m ≥ mUCH ((cid:14), δ) examples drawn i.i.d.
according to D, then, with probability of at least 1− δ, S is (cid:14)-representative.

Similar to the deﬁnition of sample complexity for PAC learning, the function
mUCH measures the (minimal) sample complexity of obtaining the uniform con-
vergence property, namely, how many examples we need to ensure that with
probability of at least 1− δ the sample would be (cid:14)-representative.
The term uniform here refers to having a ﬁxed sample size that works for all
members of H and over all possible probability distributions over the domain.

The following corollary follows directly from Lemma 4.2 and the deﬁnition of

uniform convergence.
Corollary 4.4. If a class H has the uniform convergence property with a function mUCH
then the class is agnostically PAC learnable with the sample complexity mH((cid:14), δ) ≤
mUCH ((cid:14)/2, δ). Furthermore, in that case, the ERMH paradigm is a successful agnostic
PAC learner for H.

4.2 FINITE CLASSES ARE AGNOSTIC PAC LEARNABLE
In view of Corollary 4.4, the claim that every ﬁnite hypothesis class is agnostic PAC
learnable will follow once we establish that uniform convergence holds for a ﬁnite
hypothesis class.

To show that uniform convergence holds we follow a two step argument, similar
to the derivation in Chapter 2. The ﬁrst step applies the union bound while the
second step employs a measure concentration inequality. We now explain these two
steps in detail.
Fix some (cid:14), δ. We need to ﬁnd a sample size m that guarantees that for any D,
with probability of at least 1 − δ of the choice of S = (z1, . . . , zm) sampled i.i.d. from
D we have that for all h ∈ H, |L S(h)− LD(h)| ≤ (cid:14). That is,

Dm({S : ∀h ∈ H,|L S(h)− LD(h)| ≤ (cid:14)}) ≥ 1− δ.

Equivalently, we need to show that

Dm({S : ∃h ∈ H,|L S(h)− LD(h)| > (cid:14)}) < δ.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:38:38, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.005

4.2 Finite Classes Are Agnostic PAC Learnable

33

Writing

{S : ∃h ∈ H,|L S(h)− LD(h)| > (cid:14)} = ∪h∈H{S : |L S(h)− LD(h)| > (cid:14)},

and applying the union bound (Lemma 2.2) we obtain
Dm({S : ∃h ∈ H,|L S(h)− LD(h)| > (cid:14)}) ≤

Dm({S : |L S(h)− LD(h)| > (cid:14)}).

(4.1)

(cid:7)

h∈H

Recall that LD(h) = Ez∼D [(cid:2)(h, z)] and that L S(h) = 1

Our second step will be to argue that each summand of the right-hand side of
this inequality is small enough (for a sufﬁciently large m). That is, we will show that
for any ﬁxed hypothesis, h, (which is chosen in advance prior to the sampling of the
training set), the gap between the true and empirical risks, |L S(h)− LD(h)|, is likely
to be small.
(cid:2)(h, zi ). Since each zi
is sampled i.i.d. from D, the expected value of the random variable (cid:2)(h, zi ) is LD(h).
By the linearity of expectation, it follows that LD(h) is also the expected value of
L S(h). Hence, the quantity |LD(h)− L S(h)| is the deviation of the random variable
L S(h) from its expectation. We therefore need to show that the measure of L S(h) is
concentrated around its expected value.

(cid:2)

m
i=1

m

A basic statistical fact, the law of large numbers, states that when m goes to
inﬁnity, empirical averages converge to their true expectation. This is true for L S(h),
since it is the empirical average of m i.i.d random variables. However, since the law
of large numbers is only an asymptotic result, it provides no information about the
gap between the empirically estimated error and its true value for any given, ﬁnite,
sample size.

Instead, we will use a measure concentration inequality due to Hoeffding, which

quantiﬁes the gap between empirical averages and their expected value.

(cid:15)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) > (cid:14)
(cid:2)
(cid:13)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) 1

Lemma 4.5 (Hoeffding’s Inequality). Let θ1, . . . , θm be a sequence of i.i.d. random
variables and assume that for all i, E[θi] = μ and P[a ≤ θi ≤ b] = 1. Then, for any
(cid:14) > 0

θi − μ

≤ 2 exp

(cid:16)
−2 m (cid:14)2/(b− a)2

(cid:17)

.

(cid:13)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) 1

m

P

m(cid:7)

i=1

m

m
i=1

The proof can be found in Appendix B.
Getting back to our problem, let θi be the random variable (cid:2)(h, zi ). Since h is
ﬁxed and z1, . . . , zm are sampled i.i.d., it follows that θ1, . . . , θm are also i.i.d. random
θi and LD(h) = μ. Let us further assume
variables. Furthermore, L S(h) = 1
that the range of (cid:2) is [0,1] and therefore θi ∈ [0,1]. We therefore obtain that
(cid:17)
(cid:16)
m(cid:7)
Dm({S : |L S(h)− LD(h)| > (cid:14)}) = P
−2 m (cid:14)2
(cid:17)
−2 m (cid:14)2
(cid:17)

i=1
Combining this with Equation (4.1) yields

Dm({S : ∃h ∈ H,|L S(h)− LD(h)| > (cid:14)}) ≤

≤ 2 exp
(cid:16)
(cid:16)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) > (cid:14)
(cid:7)

θi − μ

2 exp

(4.2)

(cid:15)

m

.

h∈H
= 2|H| exp

−2 m (cid:14)2

.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:38:38, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.005

34

Learning via Uniform Convergence

Finally, if we choose

then

m ≥ log(2|H|/δ)

2(cid:14)2

Dm({S : ∃h ∈ H,|L S(h)− LD(h)| > (cid:14)}) ≤ δ.

Corollary 4.6. Let H be a ﬁnite hypothesis class, let Z be a domain, and let (cid:2) : H ×
Z → [0,1] be a loss function. Then, H enjoys the uniform convergence property with
sample complexity

(cid:9)

mUCH ((cid:14), δ) ≤

log(2|H|/δ)

(cid:10)

.

Furthermore, the class is agnostically PAC learnable using the ERM algorithm with
sample complexity

2(cid:14)2

(cid:9)

(cid:10)

.

2log(2|H|/δ)

(cid:14)2

mH((cid:14), δ) ≤ mUCH ((cid:14)/2, δ) ≤

Remark 4.1 (The “Discretization Trick”). While the preceding corollary only
applies to ﬁnite hypothesis classes, there is a simple trick that allows us to get a
very good estimate of the practical sample complexity of inﬁnite hypothesis classes.
Consider a hypothesis class that is parameterized by d parameters. For example,
let X = R, Y = {±1}, and the hypothesis class, H, be all functions of the form
hθ (x) = sign(x − θ). That is, each hypothesis is parameterized by one parameter,
θ ∈ R, and the hypothesis outputs 1 for all instances larger than θ and outputs −1
for instances smaller than θ. This is a hypothesis class of an inﬁnite size. However,
if we are going to learn this hypothesis class in practice, using a computer, we will
probably maintain real numbers using ﬂoating point representation, say, of 64 bits.
It follows that in practice, our hypothesis class is parameterized by the set of scalars
that can be represented using a 64 bits ﬂoating point number. There are at most 264
such numbers; hence the actual size of our hypothesis class is at most 264. More gen-
erally, if our hypothesis class is parameterized by d numbers, in practice we learn
a hypothesis class of size at most 264d. Applying Corollary 4.6 we obtain that the
sample complexity of such classes is bounded by 128d+2 log(2/δ)
. This upper bound
on the sample complexity has the deﬁciency of being dependent on the speciﬁc rep-
resentation of real numbers used by our machine. In Chapter 6 we will introduce
a rigorous way to analyze the sample complexity of inﬁnite size hypothesis classes.
Nevertheless, the discretization trick can be used to get a rough estimate of the
sample complexity in many practical situations.

(cid:14)2

4.3 SUMMARY
If the uniform convergence property holds for a hypothesis class H then in most
cases the empirical risks of hypotheses in H will faithfully represent their true
risks. Uniform convergence sufﬁces for agnostic PAC learnability using the ERM
rule. We have shown that ﬁnite hypothesis classes enjoy the uniform convergence
property and are hence agnostic PAC learnable.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:38:38, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.005

4.5 Exercises

35

4.4 BIBLIOGRAPHIC REMARKS
Classes of functions for which the uniform convergence property holds are also
called Glivenko-Cantelli classes, named after Valery Ivanovich Glivenko and
Francesco Paolo Cantelli, who proved the ﬁrst uniform convergence result in the
1930s. See (Dudley, Gine & Zinn 1991). The relation between uniform convergence
and learnability was thoroughly studied by Vapnik – see (Vapnik 1992, Vapnik 1995,
Vapnik 1998). In fact, as we will see later in Chapter 6, the fundamental theorem of
learning theory states that in binary classiﬁcation problems, uniform convergence is
not only a sufﬁcient condition for learnability but is also a necessary condition. This
is not the case for more general learning problems (see (Shalev-Shwartz, Shamir,
Srebro & Sridharan 2010)).

4.5 EXERCISES
4.1 In this exercise, we show that the ((cid:14), δ) requirement on the convergence of errors in
our deﬁnitions of PAC learning, is, in fact, quite close to a simpler looking require-
ment about averages (or expectations). Prove that the following two statements are
equivalent (for any learning algorithm A, any probability distribution D, and any
loss function whose range is [0,1]):
1. For every (cid:14), δ > 0, there exists m((cid:14), δ) such that ∀m ≥ m((cid:14), δ)

2.

[LD(A(S)) > (cid:14)] < δ

P

S∼Dm

lim
m→∞ E
S∼Dm

[LD(A(S))]= 0

(where ES∼Dm denotes the expectation over samples S of size m).

4.2 Bounded loss functions: In Corollary 4.6 we assumed that the range of the loss func-
tion is [0,1]. Prove that if the range of the loss function is [a, b] then the sample
complexity satisﬁes

(cid:18)

mH((cid:14), δ) ≤ mUCH ((cid:14)/2, δ) ≤

2log (2|H|/δ)(b− a)2

(cid:14)2

(cid:19)

.

Downloaded from https://www.cambridge.org/core. University of Sussex Library, on 05 Mar 2019 at 20:38:38, subject to the Cambridge Core terms of use, available at
https://www.cambridge.org/core/terms. https://doi.org/10.1017/CBO9781107298019.005


